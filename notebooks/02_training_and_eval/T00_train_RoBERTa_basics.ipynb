{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319f65fe11f6c526",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68036cf48b465df",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 0.0 imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-12T08:14:00.488243Z",
     "start_time": "2025-09-12T08:13:52.722836400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 08:13:58.391801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-12 08:13:58.408583: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757664838.428214 2639735 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757664838.433557 2639735 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1757664838.448492 2639735 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757664838.448507 2639735 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757664838.448509 2639735 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1757664838.448510 2639735 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-09-12 08:13:58.454228: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_DIR = \"../../\"\n",
    "sys.path.append(BASE_DIR)\n",
    "\n",
    "from src.general_functions_and_patterns_for_detection import (\n",
    "    load_dataframe_from_json,\n",
    "    TrainRobertaHelper,\n",
    "    RESULT_DIR, CLEANED_FILES_DIR, DETECTOR_RESULTS, TASK_DIR,\n",
    "    LLMs, json_path_abstract,\n",
    "    seed_everything\n",
    ")\n",
    "\n",
    "SEED = 2023\n",
    "seed_everything(SEED)\n",
    "\n",
    "from DetectRL.Detectors.metrics import get_roc_metric_result\n",
    "\n",
    "prepare_df_for_roberta_training = TrainRobertaHelper.prepare_df_for_roberta_training\n",
    "import DetectRL.Detectors.train_roberta as train_roberta\n",
    "\n",
    "DEBUG = True\n",
    "DRY_RUN = False\n",
    "ALL_DATA = True\n",
    "RESULT_DIR = os.path.join(RESULT_DIR, \"T00\")\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6956259b0419b39",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T08:14:00.488243Z",
     "start_time": "2025-09-12T08:14:00.488243Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: adjust CUDA setup depending on your setup\n",
    "# Disable NCCL features incompatible with RTX 40xx\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "\n",
    "# Restrict to only GPU 1 (CUDA:1)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ab23d7d3cf9fa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Test Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d075ec732f965",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.0 General setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da649b282589c537",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T08:14:00.504168400Z",
     "start_time": "2025-09-12T08:14:00.488243Z"
    }
   },
   "outputs": [],
   "source": [
    "task1_path = f\"{TASK_DIR}/Task1/\"\n",
    "cleaned_file_version = \"_cleaned_final_using_dipper_v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42e4c2bc4d3853",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.1 Using the default data for comparison\n",
    "\n",
    "re-executing the multi llm generalisation test from DetectRL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a878d9c5",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "affb779dff038940",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-11T15:47:53.898411800Z",
     "start_time": "2025-09-11T08:36:48.075030400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2523, 'grad_norm': 20.64860725402832, 'learning_rate': 9.473462066054658e-07, 'epoch': 0.15827793605571383}\n",
      "{'loss': 0.0764, 'grad_norm': 0.3984912037849426, 'learning_rate': 8.945868945868945e-07, 'epoch': 0.31655587211142766}\n",
      "{'loss': 0.0264, 'grad_norm': 0.01701672002673149, 'learning_rate': 8.418275825683234e-07, 'epoch': 0.4748338081671415}\n",
      "{'loss': 0.0273, 'grad_norm': 0.014583592303097248, 'learning_rate': 7.89068270549752e-07, 'epoch': 0.6331117442228553}\n",
      "{'loss': 0.0133, 'grad_norm': 0.012019606307148933, 'learning_rate': 7.363089585311806e-07, 'epoch': 0.7913896802785692}\n",
      "{'loss': 0.0141, 'grad_norm': 0.11043582856655121, 'learning_rate': 6.835496465126095e-07, 'epoch': 0.949667616334283}\n",
      "{'eval_loss': 1.088729977607727, 'eval_accuracy': 0.8388888888888889, 'eval_f1': 0.8388888888888889, 'eval_precision': 0.8388888888888889, 'eval_recall': 0.8388888888888889, 'eval_runtime': 17.084, 'eval_samples_per_second': 42.145, 'eval_steps_per_second': 5.268, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0112, 'grad_norm': 0.005832625553011894, 'learning_rate': 6.307903344940382e-07, 'epoch': 1.1079455523899968}\n",
      "{'loss': 0.0129, 'grad_norm': 0.0030535783153027296, 'learning_rate': 5.780310224754669e-07, 'epoch': 1.2662234884457106}\n",
      "{'loss': 0.0132, 'grad_norm': 0.004167329519987106, 'learning_rate': 5.252717104568957e-07, 'epoch': 1.4245014245014245}\n",
      "{'loss': 0.0064, 'grad_norm': 0.009279988706111908, 'learning_rate': 4.7251239843832436e-07, 'epoch': 1.5827793605571383}\n",
      "{'loss': 0.0124, 'grad_norm': 0.0025347890332341194, 'learning_rate': 4.1975308641975306e-07, 'epoch': 1.7410572966128521}\n",
      "{'loss': 0.0145, 'grad_norm': 0.0016348951030522585, 'learning_rate': 3.669937744011818e-07, 'epoch': 1.899335232668566}\n",
      "{'eval_loss': 0.5793552398681641, 'eval_accuracy': 0.9222222222222223, 'eval_f1': 0.9222222222222223, 'eval_precision': 0.9222222222222223, 'eval_recall': 0.9222222222222223, 'eval_runtime': 17.0436, 'eval_samples_per_second': 42.244, 'eval_steps_per_second': 5.281, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.004, 'grad_norm': 0.0015457450645044446, 'learning_rate': 3.142344623826105e-07, 'epoch': 2.05761316872428}\n",
      "{'loss': 0.0042, 'grad_norm': 0.009953424334526062, 'learning_rate': 2.614751503640392e-07, 'epoch': 2.2158911047799936}\n",
      "{'loss': 0.0063, 'grad_norm': 0.0010754704708233476, 'learning_rate': 2.0871583834546797e-07, 'epoch': 2.3741690408357075}\n",
      "{'loss': 0.0054, 'grad_norm': 0.0015438495902344584, 'learning_rate': 1.5595652632689667e-07, 'epoch': 2.5324469768914213}\n",
      "{'loss': 0.0027, 'grad_norm': 0.0012529799714684486, 'learning_rate': 1.0319721430832541e-07, 'epoch': 2.690724912947135}\n",
      "{'loss': 0.0018, 'grad_norm': 0.0017474978230893612, 'learning_rate': 5.043790228975414e-08, 'epoch': 2.849002849002849}\n",
      "{'eval_loss': 0.8853952884674072, 'eval_accuracy': 0.8916666666666667, 'eval_f1': 0.8916666666666667, 'eval_precision': 0.8916666666666667, 'eval_recall': 0.8916666666666667, 'eval_runtime': 17.108, 'eval_samples_per_second': 42.086, 'eval_steps_per_second': 5.261, 'epoch': 3.0}\n",
      "{'train_runtime': 6229.0201, 'train_samples_per_second': 12.169, 'train_steps_per_second': 1.521, 'train_loss': 0.02674183923382793, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8853952884674072, 'eval_accuracy': 0.8916666666666667, 'eval_f1': 0.8916666666666667, 'eval_precision': 0.8916666666666667, 'eval_recall': 0.8916666666666667, 'eval_runtime': 16.8549, 'eval_samples_per_second': 42.718, 'eval_steps_per_second': 5.34, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      " 25%|██▌       | 1/4 [1:47:57<5:23:52, 6477.34s/it]/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3108, 'grad_norm': 9.832615852355957, 'learning_rate': 9.473462066054658e-07, 'epoch': 0.15827793605571383}\n",
      "{'loss': 0.1563, 'grad_norm': 0.11783894896507263, 'learning_rate': 8.945868945868945e-07, 'epoch': 0.31655587211142766}\n",
      "{'loss': 0.0658, 'grad_norm': 11.575504302978516, 'learning_rate': 8.418275825683234e-07, 'epoch': 0.4748338081671415}\n",
      "{'loss': 0.0395, 'grad_norm': 13.838899612426758, 'learning_rate': 7.89068270549752e-07, 'epoch': 0.6331117442228553}\n",
      "{'loss': 0.0281, 'grad_norm': 0.04161103069782257, 'learning_rate': 7.363089585311806e-07, 'epoch': 0.7913896802785692}\n",
      "{'loss': 0.0229, 'grad_norm': 0.039605092257261276, 'learning_rate': 6.835496465126095e-07, 'epoch': 0.949667616334283}\n",
      "{'eval_loss': 0.21092259883880615, 'eval_accuracy': 0.9597222222222223, 'eval_f1': 0.9597222222222223, 'eval_precision': 0.9597222222222223, 'eval_recall': 0.9597222222222223, 'eval_runtime': 17.0047, 'eval_samples_per_second': 42.341, 'eval_steps_per_second': 5.293, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0158, 'grad_norm': 0.008376765996217728, 'learning_rate': 6.307903344940382e-07, 'epoch': 1.1079455523899968}\n",
      "{'loss': 0.0122, 'grad_norm': 0.01293924544006586, 'learning_rate': 5.780310224754669e-07, 'epoch': 1.2662234884457106}\n",
      "{'loss': 0.0142, 'grad_norm': 0.009533166885375977, 'learning_rate': 5.252717104568957e-07, 'epoch': 1.4245014245014245}\n",
      "{'loss': 0.0262, 'grad_norm': 0.018696915358304977, 'learning_rate': 4.7251239843832436e-07, 'epoch': 1.5827793605571383}\n",
      "{'loss': 0.01, 'grad_norm': 0.004438058473169804, 'learning_rate': 4.1975308641975306e-07, 'epoch': 1.7410572966128521}\n",
      "{'loss': 0.0067, 'grad_norm': 0.007136004976928234, 'learning_rate': 3.669937744011818e-07, 'epoch': 1.899335232668566}\n",
      "{'eval_loss': 0.10678107291460037, 'eval_accuracy': 0.9819444444444444, 'eval_f1': 0.9819444444444444, 'eval_precision': 0.9819444444444444, 'eval_recall': 0.9819444444444444, 'eval_runtime': 17.0069, 'eval_samples_per_second': 42.336, 'eval_steps_per_second': 5.292, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0069, 'grad_norm': 0.02365836128592491, 'learning_rate': 3.142344623826105e-07, 'epoch': 2.05761316872428}\n",
      "{'loss': 0.0127, 'grad_norm': 0.7525432705879211, 'learning_rate': 2.614751503640392e-07, 'epoch': 2.2158911047799936}\n",
      "{'loss': 0.0063, 'grad_norm': 0.0023267697542905807, 'learning_rate': 2.0871583834546797e-07, 'epoch': 2.3741690408357075}\n",
      "{'loss': 0.0098, 'grad_norm': 0.004604933317750692, 'learning_rate': 1.5595652632689667e-07, 'epoch': 2.5324469768914213}\n",
      "{'loss': 0.0065, 'grad_norm': 0.002717529656365514, 'learning_rate': 1.0319721430832541e-07, 'epoch': 2.690724912947135}\n",
      "{'loss': 0.005, 'grad_norm': 0.005048414226621389, 'learning_rate': 5.043790228975414e-08, 'epoch': 2.849002849002849}\n",
      "{'eval_loss': 0.1696123480796814, 'eval_accuracy': 0.9694444444444444, 'eval_f1': 0.9694444444444444, 'eval_precision': 0.9694444444444444, 'eval_recall': 0.9694444444444444, 'eval_runtime': 17.0486, 'eval_samples_per_second': 42.232, 'eval_steps_per_second': 5.279, 'epoch': 3.0}\n",
      "{'train_runtime': 6220.0489, 'train_samples_per_second': 12.188, 'train_steps_per_second': 1.524, 'train_loss': 0.04035255377385968, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1696123480796814, 'eval_accuracy': 0.9694444444444444, 'eval_f1': 0.9694444444444444, 'eval_precision': 0.9694444444444444, 'eval_recall': 0.9694444444444444, 'eval_runtime': 16.826, 'eval_samples_per_second': 42.791, 'eval_steps_per_second': 5.349, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      " 50%|█████     | 2/4 [3:35:46<3:35:44, 6472.42s/it]/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.294, 'grad_norm': 8.698787689208984, 'learning_rate': 9.473462066054658e-07, 'epoch': 0.15827793605571383}\n",
      "{'loss': 0.1849, 'grad_norm': 5.566482067108154, 'learning_rate': 8.945868945868945e-07, 'epoch': 0.31655587211142766}\n",
      "{'loss': 0.0753, 'grad_norm': 0.9402514696121216, 'learning_rate': 8.418275825683234e-07, 'epoch': 0.4748338081671415}\n",
      "{'loss': 0.0328, 'grad_norm': 0.022916847839951515, 'learning_rate': 7.89068270549752e-07, 'epoch': 0.6331117442228553}\n",
      "{'loss': 0.0252, 'grad_norm': 0.03137892857193947, 'learning_rate': 7.363089585311806e-07, 'epoch': 0.7913896802785692}\n",
      "{'loss': 0.0286, 'grad_norm': 0.010812385939061642, 'learning_rate': 6.835496465126095e-07, 'epoch': 0.949667616334283}\n",
      "{'eval_loss': 1.5617460012435913, 'eval_accuracy': 0.7527777777777778, 'eval_f1': 0.7527777777777778, 'eval_precision': 0.7527777777777778, 'eval_recall': 0.7527777777777778, 'eval_runtime': 17.0089, 'eval_samples_per_second': 42.331, 'eval_steps_per_second': 5.291, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0211, 'grad_norm': 0.015292403288185596, 'learning_rate': 6.307903344940382e-07, 'epoch': 1.1079455523899968}\n",
      "{'loss': 0.0138, 'grad_norm': 0.01023515872657299, 'learning_rate': 5.780310224754669e-07, 'epoch': 1.2662234884457106}\n",
      "{'loss': 0.0127, 'grad_norm': 0.004542579874396324, 'learning_rate': 5.252717104568957e-07, 'epoch': 1.4245014245014245}\n",
      "{'loss': 0.0179, 'grad_norm': 0.019629422575235367, 'learning_rate': 4.7251239843832436e-07, 'epoch': 1.5827793605571383}\n",
      "{'loss': 0.0128, 'grad_norm': 0.002932896139100194, 'learning_rate': 4.1975308641975306e-07, 'epoch': 1.7410572966128521}\n",
      "{'loss': 0.0101, 'grad_norm': 0.003935432527214289, 'learning_rate': 3.669937744011818e-07, 'epoch': 1.899335232668566}\n",
      "{'eval_loss': 1.8899675607681274, 'eval_accuracy': 0.7527777777777778, 'eval_f1': 0.7527777777777778, 'eval_precision': 0.7527777777777778, 'eval_recall': 0.7527777777777778, 'eval_runtime': 16.996, 'eval_samples_per_second': 42.363, 'eval_steps_per_second': 5.295, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0179, 'grad_norm': 0.0021579728927463293, 'learning_rate': 3.142344623826105e-07, 'epoch': 2.05761316872428}\n",
      "{'loss': 0.0072, 'grad_norm': 0.002422079909592867, 'learning_rate': 2.614751503640392e-07, 'epoch': 2.2158911047799936}\n",
      "{'loss': 0.0089, 'grad_norm': 0.002113007241860032, 'learning_rate': 2.0871583834546797e-07, 'epoch': 2.3741690408357075}\n",
      "{'loss': 0.0173, 'grad_norm': 0.002230482641607523, 'learning_rate': 1.5595652632689667e-07, 'epoch': 2.5324469768914213}\n",
      "{'loss': 0.0046, 'grad_norm': 0.0014976415550336242, 'learning_rate': 1.0319721430832541e-07, 'epoch': 2.690724912947135}\n",
      "{'loss': 0.002, 'grad_norm': 0.0021020725835114717, 'learning_rate': 5.043790228975414e-08, 'epoch': 2.849002849002849}\n",
      "{'eval_loss': 1.537498116493225, 'eval_accuracy': 0.8111111111111111, 'eval_f1': 0.8111111111111111, 'eval_precision': 0.8111111111111111, 'eval_recall': 0.8111111111111111, 'eval_runtime': 16.9673, 'eval_samples_per_second': 42.435, 'eval_steps_per_second': 5.304, 'epoch': 3.0}\n",
      "{'train_runtime': 6236.867, 'train_samples_per_second': 12.154, 'train_steps_per_second': 1.52, 'train_loss': 0.04189492322302473, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.537498116493225, 'eval_accuracy': 0.8111111111111111, 'eval_f1': 0.8111111111111111, 'eval_precision': 0.8111111111111111, 'eval_recall': 0.8111111111111111, 'eval_runtime': 16.7377, 'eval_samples_per_second': 43.017, 'eval_steps_per_second': 5.377, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      " 75%|███████▌  | 3/4 [5:23:53<1:47:59, 6479.25s/it]/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2996, 'grad_norm': 5.264007568359375, 'learning_rate': 9.473462066054658e-07, 'epoch': 0.15827793605571383}\n",
      "{'loss': 0.2293, 'grad_norm': 6.263920783996582, 'learning_rate': 8.945868945868945e-07, 'epoch': 0.31655587211142766}\n",
      "{'loss': 0.1656, 'grad_norm': 0.7451289296150208, 'learning_rate': 8.418275825683234e-07, 'epoch': 0.4748338081671415}\n",
      "{'loss': 0.0708, 'grad_norm': 0.12793347239494324, 'learning_rate': 7.89068270549752e-07, 'epoch': 0.6331117442228553}\n",
      "{'loss': 0.0461, 'grad_norm': 0.022311659529805183, 'learning_rate': 7.363089585311806e-07, 'epoch': 0.7913896802785692}\n",
      "{'loss': 0.0442, 'grad_norm': 0.013789534568786621, 'learning_rate': 6.835496465126095e-07, 'epoch': 0.949667616334283}\n",
      "{'eval_loss': 2.531515121459961, 'eval_accuracy': 0.6236111111111111, 'eval_f1': 0.6236111111111111, 'eval_precision': 0.6236111111111111, 'eval_recall': 0.6236111111111111, 'eval_runtime': 16.9965, 'eval_samples_per_second': 42.362, 'eval_steps_per_second': 5.295, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0343, 'grad_norm': 0.016003813594579697, 'learning_rate': 6.307903344940382e-07, 'epoch': 1.1079455523899968}\n",
      "{'loss': 0.037, 'grad_norm': 0.007550846319645643, 'learning_rate': 5.780310224754669e-07, 'epoch': 1.2662234884457106}\n",
      "{'loss': 0.0353, 'grad_norm': 0.007808472495526075, 'learning_rate': 5.252717104568957e-07, 'epoch': 1.4245014245014245}\n",
      "{'loss': 0.0338, 'grad_norm': 0.04921482503414154, 'learning_rate': 4.7251239843832436e-07, 'epoch': 1.5827793605571383}\n",
      "{'loss': 0.0227, 'grad_norm': 0.00774453766644001, 'learning_rate': 4.1975308641975306e-07, 'epoch': 1.7410572966128521}\n",
      "{'loss': 0.0229, 'grad_norm': 0.003965183161199093, 'learning_rate': 3.669937744011818e-07, 'epoch': 1.899335232668566}\n",
      "{'eval_loss': 2.121584415435791, 'eval_accuracy': 0.7222222222222222, 'eval_f1': 0.7222222222222222, 'eval_precision': 0.7222222222222222, 'eval_recall': 0.7222222222222222, 'eval_runtime': 16.9932, 'eval_samples_per_second': 42.37, 'eval_steps_per_second': 5.296, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0196, 'grad_norm': 0.004914296790957451, 'learning_rate': 3.142344623826105e-07, 'epoch': 2.05761316872428}\n",
      "{'loss': 0.0164, 'grad_norm': 0.004122857004404068, 'learning_rate': 2.614751503640392e-07, 'epoch': 2.2158911047799936}\n",
      "{'loss': 0.0173, 'grad_norm': 0.004774910863488913, 'learning_rate': 2.0871583834546797e-07, 'epoch': 2.3741690408357075}\n",
      "{'loss': 0.0313, 'grad_norm': 0.005968223791569471, 'learning_rate': 1.5595652632689667e-07, 'epoch': 2.5324469768914213}\n",
      "{'loss': 0.0146, 'grad_norm': 0.004050179850310087, 'learning_rate': 1.0319721430832541e-07, 'epoch': 2.690724912947135}\n",
      "{'loss': 0.0261, 'grad_norm': 0.005742188543081284, 'learning_rate': 5.043790228975414e-08, 'epoch': 2.849002849002849}\n",
      "{'eval_loss': 2.2390310764312744, 'eval_accuracy': 0.7111111111111111, 'eval_f1': 0.7111111111111111, 'eval_precision': 0.7111111111111111, 'eval_recall': 0.7111111111111111, 'eval_runtime': 16.9825, 'eval_samples_per_second': 42.397, 'eval_steps_per_second': 5.3, 'epoch': 3.0}\n",
      "{'train_runtime': 6200.183, 'train_samples_per_second': 12.226, 'train_steps_per_second': 1.529, 'train_loss': 0.06246691795671944, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.2390310764312744, 'eval_accuracy': 0.7111111111111111, 'eval_f1': 0.7111111111111111, 'eval_precision': 0.7111111111111111, 'eval_recall': 0.7111111111111111, 'eval_runtime': 16.7484, 'eval_samples_per_second': 42.989, 'eval_steps_per_second': 5.374, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "100%|██████████| 4/4 [7:11:05<00:00, 6466.46s/it]  \n"
     ]
    }
   ],
   "source": [
    "for _llm in tqdm(LLMs):\n",
    "    other_LLMs = list(set(LLMs) - {_llm})\n",
    "    if DRY_RUN:\n",
    "        _llm += \"_dry_run\"\n",
    "        other_LLMs = [item + \"_dry_run\" for item in other_LLMs]\n",
    "    appendix = \"_all_data\" if ALL_DATA else \"\"\n",
    "\n",
    "    # --- Set parameters in a dict ---\n",
    "    args_dict = {\n",
    "        \"model_name\": \"roberta-base\",\n",
    "        \"save_model_path\": f\"{DETECTOR_RESULTS}/roberta_base_classifier_{_llm}{appendix}\",\n",
    "        \"train_data_path\": f\"{task1_path}/multi_llms_{_llm}_train.json\",\n",
    "        \"test_data_path\": f\"{task1_path}/multi_llms_{_llm}_test.json\",\n",
    "        \"transfer_test_data_path\": f\"{task1_path}/multi_llms_{other_LLMs[0]}_test.json,\" \\\n",
    "                                   f\"{task1_path}/multi_llms_{other_LLMs[1]}_test.json,\" \\\n",
    "                                   f\"{task1_path}/multi_llms_{other_LLMs[2]}_test.json\",\n",
    "        \"train_df\": None,\n",
    "        \"test_df\": None,\n",
    "        \"transfer_df\": None,\n",
    "        \"epochs\": 3,\n",
    "        \"learning_rate\": 1e-6,\n",
    "        \"batch_size\": 8,\n",
    "        \"seed\": 2023,\n",
    "        \"mode\": \"train\",\n",
    "        \"DEVICE\": \"cuda\"\n",
    "    }\n",
    "\n",
    "    # Convert dict to namespace-like object\n",
    "    args = SimpleNamespace(**args_dict)\n",
    "\n",
    "    # Call the run function\n",
    "    train_roberta.run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b7a11c",
   "metadata": {},
   "source": [
    "### combine training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f45573a87c8176",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-11T15:47:53.929893400Z",
     "start_time": "2025-09-11T15:47:53.898411800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                             roc_auc  optimal_threshold  precision    recall  \\\nllm_type_used_for_training                                                     \nChatGPT                     0.980756          -0.637154   0.934586  0.933862   \nClaude-instant              0.992980          -0.668812   0.950746  0.977844   \nGoogle-PaLM                 0.993473          -0.000087   0.972141  0.957341   \nLlama-2-70b                 0.992162          -0.000052   0.974823  0.948413   \n\n                                  f1  accuracy  tpr_at_fpr_0_01  \nllm_type_used_for_training                                       \nChatGPT                     0.934074  0.933931         0.544643  \nClaude-instant              0.964018  0.963313         0.679233  \nGoogle-PaLM                 0.964675  0.964807         0.668320  \nLlama-2-70b                 0.961273  0.961985         0.574074  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>roc_auc</th>\n      <th>optimal_threshold</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1</th>\n      <th>accuracy</th>\n      <th>tpr_at_fpr_0_01</th>\n    </tr>\n    <tr>\n      <th>llm_type_used_for_training</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ChatGPT</th>\n      <td>0.980756</td>\n      <td>-0.637154</td>\n      <td>0.934586</td>\n      <td>0.933862</td>\n      <td>0.934074</td>\n      <td>0.933931</td>\n      <td>0.544643</td>\n    </tr>\n    <tr>\n      <th>Claude-instant</th>\n      <td>0.992980</td>\n      <td>-0.668812</td>\n      <td>0.950746</td>\n      <td>0.977844</td>\n      <td>0.964018</td>\n      <td>0.963313</td>\n      <td>0.679233</td>\n    </tr>\n    <tr>\n      <th>Google-PaLM</th>\n      <td>0.993473</td>\n      <td>-0.000087</td>\n      <td>0.972141</td>\n      <td>0.957341</td>\n      <td>0.964675</td>\n      <td>0.964807</td>\n      <td>0.668320</td>\n    </tr>\n    <tr>\n      <th>Llama-2-70b</th>\n      <td>0.992162</td>\n      <td>-0.000052</td>\n      <td>0.974823</td>\n      <td>0.948413</td>\n      <td>0.961273</td>\n      <td>0.961985</td>\n      <td>0.574074</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for llm in LLMs:\n",
    "    other_LLMs = list(set(LLMs) - {llm})\n",
    "    classifier_name = \"roberta-base\"\n",
    "    directory_results = f\"{DETECTOR_RESULTS}roberta_base_classifier_{llm}{appendix}\"\n",
    "    # Transfer test sets for the 3 other LLMs\n",
    "    results = []\n",
    "    for other in other_LLMs:\n",
    "        result_file = os.path.join(directory_results, f\"multi_llms_{other}_test.json.{classifier_name}_result.json\")\n",
    "        with open(result_file, \"r\") as fp:\n",
    "            results.append(json.load(fp))\n",
    "    averaged_results_df = pd.DataFrame(results)\n",
    "    averaged_results_df.mean(numeric_only=True)\n",
    "    averaged_results_df[\"llm_type_used_for_training\"] = llm\n",
    "    dfs.append(averaged_results_df)\n",
    "\n",
    "df_combined = pd.concat(dfs)\n",
    "df_combined.groupby(by=[\"llm_type_used_for_training\"]).mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "{'ChatGPT': -2.456542097206693e-05,\n 'Claude-instant': -7.507131522288546e-05,\n 'Google-PaLM': -7.56011504563503e-05,\n 'Llama-2-70b': -3.26035515172407e-05}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for llm in LLMs:\n",
    "    classifier_name = \"roberta-base\"\n",
    "    directory_results = f\"{DETECTOR_RESULTS}roberta_base_classifier_{llm}{appendix}\"\n",
    "    result_file = os.path.join(directory_results, f\"multi_llms_{llm}_test.json.{classifier_name}_result.json\")\n",
    "    with open(result_file, \"r\") as fp:\n",
    "        results = [json.load(fp)]\n",
    "    averaged_results_df = pd.DataFrame(results)\n",
    "    averaged_results_df[\"llm_type_used_for_training\"] = llm\n",
    "    dfs.append(averaged_results_df)\n",
    "\n",
    "df_combined = pd.concat(dfs)\n",
    "df_combined = df_combined.groupby(by=[\"llm_type_used_for_training\"]).mean(numeric_only=True)\n",
    "optimal_thresholds = df_combined[\"optimal_threshold\"].to_dict()\n",
    "optimal_thresholds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-11T16:05:23.437134400Z",
     "start_time": "2025-09-11T16:05:23.405542300Z"
    }
   },
   "id": "ecc26c93c792b40c"
  },
  {
   "cell_type": "markdown",
   "id": "0d1eb5a6",
   "metadata": {},
   "source": [
    "### Further evaluation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d3ce4c3a827b3c3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-11T16:26:25.472703500Z",
     "start_time": "2025-09-11T16:05:36.120608100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2008 [00:00<?, ?it/s]/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "100%|██████████| 2008/2008 [01:35<00:00, 21.07it/s]\n",
      "  0%|          | 0/2008 [00:00<?, ?it/s]/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "100%|██████████| 2008/2008 [01:39<00:00, 20.15it/s]\n",
      "  0%|          | 0/2008 [00:00<?, ?it/s]/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "100%|██████████| 2008/2008 [01:44<00:00, 19.19it/s]\n",
      "  0%|          | 0/2008 [00:00<?, ?it/s]/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "100%|██████████| 2008/2008 [01:49<00:00, 18.37it/s]\n",
      "  0%|          | 0/2008 [00:00<?, ?it/s]/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "100%|██████████| 2008/2008 [01:42<00:00, 19.68it/s]\n",
      "  0%|          | 0/2008 [00:00<?, ?it/s]/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "100%|██████████| 2008/2008 [01:45<00:00, 19.06it/s]\n",
      "  0%|          | 0/2008 [00:00<?, ?it/s]/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "100%|██████████| 2008/2008 [01:49<00:00, 18.41it/s]\n",
      "  0%|          | 0/2008 [00:00<?, ?it/s]/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "100%|██████████| 2008/2008 [01:42<00:00, 19.61it/s]\n",
      "  0%|          | 0/2008 [00:00<?, ?it/s]/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "100%|██████████| 2008/2008 [01:41<00:00, 19.77it/s]\n",
      "  0%|          | 0/2008 [00:00<?, ?it/s]/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "100%|██████████| 2008/2008 [01:49<00:00, 18.37it/s]\n",
      "  0%|          | 0/2008 [00:00<?, ?it/s]/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "100%|██████████| 2008/2008 [01:42<00:00, 19.61it/s]\n",
      "  0%|          | 0/2008 [00:00<?, ?it/s]/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "100%|██████████| 2008/2008 [01:45<00:00, 19.02it/s]\n"
     ]
    }
   ],
   "source": [
    "combined_results = []\n",
    "\n",
    "for _llm in LLMs:\n",
    "\n",
    "    directory_results = f\"{DETECTOR_RESULTS}roberta_base_classifier_{_llm}_all_data\"\n",
    "\n",
    "    detector = transformers.AutoModelForSequenceClassification.from_pretrained(directory_results).to(\"cuda\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(directory_results)\n",
    "    other_LLMs = sorted(list(set(LLMs) - {_llm}))\n",
    "    args_experiment_dict = {\n",
    "        \"model_name\": directory_results,\n",
    "        \"test_data_path\": f\"{task1_path}/multi_llms_{other_LLMs[0]}_test.json,\" \\\n",
    "                          f\"{task1_path}/multi_llms_{other_LLMs[1]}_test.json,\" \\\n",
    "                          f\"{task1_path}/multi_llms_{other_LLMs[2]}_test.json\",\n",
    "        \"seed\": 2023,\n",
    "        \"DEVICE\": \"cuda\"\n",
    "    }\n",
    "\n",
    "    # Convert dict to namespace-like object\n",
    "    args = SimpleNamespace(**args_experiment_dict)\n",
    "\n",
    "    filenames = args.test_data_path.split(\",\")\n",
    "    results = []\n",
    "\n",
    "    for filename in filenames:\n",
    "        logging.info(f\"Test in {filename}\")\n",
    "        test_data = json.load(open(filename, \"r\"))\n",
    "\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "\n",
    "        predictions = {'human': [], 'llm': []}\n",
    "        with torch.no_grad():\n",
    "            for item in tqdm(test_data):\n",
    "                text = item[\"text\"]\n",
    "                label = item[\"label\"]\n",
    "\n",
    "                if label == \"human\":\n",
    "                    tokenized = tokenizer([text], padding=True, truncation=True, max_length=512,\n",
    "                                          return_tensors=\"pt\").to(args.DEVICE)\n",
    "                    predictions[\"human\"].append(detector(**tokenized).logits.softmax(-1)[:, 0].tolist()[0])\n",
    "                    item[\"prediction\"] = detector(**tokenized).logits.softmax(-1)[:, 0].tolist()[0]\n",
    "                elif label == \"llm\":\n",
    "                    tokenized = tokenizer([text], padding=True, truncation=True, max_length=512,\n",
    "                                          return_tensors=\"pt\").to(args.DEVICE)\n",
    "                    predictions[\"llm\"].append(detector(**tokenized).logits.softmax(-1)[:, 0].tolist()[0])\n",
    "                    item[\"prediction\"] = detector(**tokenized).logits.softmax(-1)[:, 0].tolist()[0]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown label {label}\")\n",
    "\n",
    "        predictions['human'] = [-i for i in predictions['human'] if np.isfinite(i)]\n",
    "        predictions['llm'] = [-i for i in predictions['llm'] if np.isfinite(i)]\n",
    "\n",
    "        result = get_roc_metric_result(predictions['human'], predictions['llm'], optimal_thresholds[_llm])\n",
    "        result[\"llm_used_for_training\"] = _llm\n",
    "        result[\"evaluation_file_path\"] = filename\n",
    "\n",
    "        if \"xlm-roberta-base\" in args.model_name:\n",
    "            result[\"model_type\"] = \"xlm-roberta-base\"\n",
    "        if \"xlm-roberta-large\" in args.model_name:\n",
    "            result[\"model_type\"] = \"xlm-roberta-large\"\n",
    "\n",
    "        results.append(result)\n",
    "        logging.info(f\"{result}\")\n",
    "        with open(directory_results + f\"{filename.split(\"/\")[-1].split(\".json\")[0]}_evaluation_data.json\", \"w\") as f:\n",
    "            json.dump(test_data, f, indent=4)\n",
    "\n",
    "        with open(directory_results + f\"{filename.split(\"/\")[-1].split(\".json\")[0]}_evaluation_results.json\", \"w\") as f:\n",
    "            json.dump(result, f, indent=4)\n",
    "\n",
    "    combined_results.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc8a71af04f93a43",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-11T16:26:25.562073300Z",
     "start_time": "2025-09-11T16:26:25.478223700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     roc_auc  optimal_threshold              conf_matrix  precision    recall  \\\n0   0.972107          -0.000025   [[999, 1], [574, 434]]   0.997701  0.430556   \n1   0.972681          -0.000025  [[1000, 0], [484, 524]]   1.000000  0.519841   \n2   0.997481          -0.000025   [[999, 1], [180, 828]]   0.998794  0.821429   \n3   0.998903          -0.000075    [[996, 4], [65, 943]]   0.995776  0.935516   \n4   0.988103          -0.000075   [[998, 2], [261, 747]]   0.997330  0.741071   \n5   0.991935          -0.000075   [[992, 8], [211, 797]]   0.990062  0.790675   \n6   0.998813          -0.000033    [[993, 7], [8, 1000]]   0.993049  0.992063   \n7   0.984159          -0.000033   [[998, 2], [422, 586]]   0.996599  0.581349   \n8   0.993513          -0.000033   [[999, 1], [242, 766]]   0.998696  0.759921   \n9   0.998326          -0.000076   [[976, 24], [8, 1000]]   0.976562  0.992063   \n10  0.984895          -0.000076  [[980, 20], [174, 834]]   0.976581  0.827381   \n11  0.997198          -0.000076   [[983, 17], [42, 966]]   0.982706  0.958333   \n\n          f1  accuracy  tpr_at_fpr_0_01 llm_used_for_training  \\\n0   0.601525  0.713645         0.287698               ChatGPT   \n1   0.684073  0.758964         0.534722               ChatGPT   \n2   0.901470  0.909861         0.811508               ChatGPT   \n3   0.964706  0.965637         0.885913        Claude-instant   \n4   0.850313  0.869024         0.501984        Claude-instant   \n5   0.879206  0.890936         0.649802        Claude-instant   \n6   0.992556  0.992530         0.527778           Llama-2-70b   \n7   0.734336  0.788845         0.486111           Llama-2-70b   \n8   0.863099  0.878984         0.708333           Llama-2-70b   \n9   0.984252  0.984064         0.866071           Google-PaLM   \n10  0.895811  0.903386         0.319444           Google-PaLM   \n11  0.970367  0.970618         0.819444           Google-PaLM   \n\n                        eval_model  \n0   multi_llms_Claude-instant_test  \n1      multi_llms_Google-PaLM_test  \n2      multi_llms_Llama-2-70b_test  \n3          multi_llms_ChatGPT_test  \n4      multi_llms_Google-PaLM_test  \n5      multi_llms_Llama-2-70b_test  \n6          multi_llms_ChatGPT_test  \n7   multi_llms_Claude-instant_test  \n8      multi_llms_Google-PaLM_test  \n9          multi_llms_ChatGPT_test  \n10  multi_llms_Claude-instant_test  \n11     multi_llms_Llama-2-70b_test  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>roc_auc</th>\n      <th>optimal_threshold</th>\n      <th>conf_matrix</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1</th>\n      <th>accuracy</th>\n      <th>tpr_at_fpr_0_01</th>\n      <th>llm_used_for_training</th>\n      <th>eval_model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.972107</td>\n      <td>-0.000025</td>\n      <td>[[999, 1], [574, 434]]</td>\n      <td>0.997701</td>\n      <td>0.430556</td>\n      <td>0.601525</td>\n      <td>0.713645</td>\n      <td>0.287698</td>\n      <td>ChatGPT</td>\n      <td>multi_llms_Claude-instant_test</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.972681</td>\n      <td>-0.000025</td>\n      <td>[[1000, 0], [484, 524]]</td>\n      <td>1.000000</td>\n      <td>0.519841</td>\n      <td>0.684073</td>\n      <td>0.758964</td>\n      <td>0.534722</td>\n      <td>ChatGPT</td>\n      <td>multi_llms_Google-PaLM_test</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.997481</td>\n      <td>-0.000025</td>\n      <td>[[999, 1], [180, 828]]</td>\n      <td>0.998794</td>\n      <td>0.821429</td>\n      <td>0.901470</td>\n      <td>0.909861</td>\n      <td>0.811508</td>\n      <td>ChatGPT</td>\n      <td>multi_llms_Llama-2-70b_test</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.998903</td>\n      <td>-0.000075</td>\n      <td>[[996, 4], [65, 943]]</td>\n      <td>0.995776</td>\n      <td>0.935516</td>\n      <td>0.964706</td>\n      <td>0.965637</td>\n      <td>0.885913</td>\n      <td>Claude-instant</td>\n      <td>multi_llms_ChatGPT_test</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.988103</td>\n      <td>-0.000075</td>\n      <td>[[998, 2], [261, 747]]</td>\n      <td>0.997330</td>\n      <td>0.741071</td>\n      <td>0.850313</td>\n      <td>0.869024</td>\n      <td>0.501984</td>\n      <td>Claude-instant</td>\n      <td>multi_llms_Google-PaLM_test</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.991935</td>\n      <td>-0.000075</td>\n      <td>[[992, 8], [211, 797]]</td>\n      <td>0.990062</td>\n      <td>0.790675</td>\n      <td>0.879206</td>\n      <td>0.890936</td>\n      <td>0.649802</td>\n      <td>Claude-instant</td>\n      <td>multi_llms_Llama-2-70b_test</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.998813</td>\n      <td>-0.000033</td>\n      <td>[[993, 7], [8, 1000]]</td>\n      <td>0.993049</td>\n      <td>0.992063</td>\n      <td>0.992556</td>\n      <td>0.992530</td>\n      <td>0.527778</td>\n      <td>Llama-2-70b</td>\n      <td>multi_llms_ChatGPT_test</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.984159</td>\n      <td>-0.000033</td>\n      <td>[[998, 2], [422, 586]]</td>\n      <td>0.996599</td>\n      <td>0.581349</td>\n      <td>0.734336</td>\n      <td>0.788845</td>\n      <td>0.486111</td>\n      <td>Llama-2-70b</td>\n      <td>multi_llms_Claude-instant_test</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.993513</td>\n      <td>-0.000033</td>\n      <td>[[999, 1], [242, 766]]</td>\n      <td>0.998696</td>\n      <td>0.759921</td>\n      <td>0.863099</td>\n      <td>0.878984</td>\n      <td>0.708333</td>\n      <td>Llama-2-70b</td>\n      <td>multi_llms_Google-PaLM_test</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.998326</td>\n      <td>-0.000076</td>\n      <td>[[976, 24], [8, 1000]]</td>\n      <td>0.976562</td>\n      <td>0.992063</td>\n      <td>0.984252</td>\n      <td>0.984064</td>\n      <td>0.866071</td>\n      <td>Google-PaLM</td>\n      <td>multi_llms_ChatGPT_test</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.984895</td>\n      <td>-0.000076</td>\n      <td>[[980, 20], [174, 834]]</td>\n      <td>0.976581</td>\n      <td>0.827381</td>\n      <td>0.895811</td>\n      <td>0.903386</td>\n      <td>0.319444</td>\n      <td>Google-PaLM</td>\n      <td>multi_llms_Claude-instant_test</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.997198</td>\n      <td>-0.000076</td>\n      <td>[[983, 17], [42, 966]]</td>\n      <td>0.982706</td>\n      <td>0.958333</td>\n      <td>0.970367</td>\n      <td>0.970618</td>\n      <td>0.819444</td>\n      <td>Google-PaLM</td>\n      <td>multi_llms_Llama-2-70b_test</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(combined_results)\n",
    "results_df[\"eval_model\"] = results_df[\"evaluation_file_path\"].apply(lambda x: x.split(\"/\")[-1].split(\".\")[0])\n",
    "results_df.drop(columns=[\"evaluation_file_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa95ca96c417db1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Only train for Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a9f7aae628382",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.1 Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4262df9058de8e2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T08:14:12.495619700Z",
     "start_time": "2025-09-12T08:14:12.164329600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        id                                            context        llm_type  \\\n1400  1401  Real Time Turbulent Video Perfecting by Image ...  Claude-instant   \n1401  1402   Finite Euler products and the Riemann Hypothesis  Claude-instant   \n1402  1403  An Adaptive Strategy for the Classification of...  Claude-instant   \n1403  1404  Detailed Models of super-Earths: How well can ...  Claude-instant   \n1404  1405    The Distribution of AGN in Clusters of Galaxies  Claude-instant   \n\n                                                   text domain  label  \\\n1400  Image and video quality in Long Range Observat...  arxiv  human   \n1401  We show that if the Riemann Hypothesis is true...  arxiv  human   \n1402  One of the major problems in computational bio...  arxiv  human   \n1403  The field of extrasolar planets has rapidly ex...  arxiv  human   \n1404  We present a study of the distribution of AGN ...  arxiv  human   \n\n     llm_prompting_strategy  \n1400          direct_prompt  \n1401          direct_prompt  \n1402          direct_prompt  \n1403          direct_prompt  \n1404          direct_prompt  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>context</th>\n      <th>llm_type</th>\n      <th>text</th>\n      <th>domain</th>\n      <th>label</th>\n      <th>llm_prompting_strategy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1400</th>\n      <td>1401</td>\n      <td>Real Time Turbulent Video Perfecting by Image ...</td>\n      <td>Claude-instant</td>\n      <td>Image and video quality in Long Range Observat...</td>\n      <td>arxiv</td>\n      <td>human</td>\n      <td>direct_prompt</td>\n    </tr>\n    <tr>\n      <th>1401</th>\n      <td>1402</td>\n      <td>Finite Euler products and the Riemann Hypothesis</td>\n      <td>Claude-instant</td>\n      <td>We show that if the Riemann Hypothesis is true...</td>\n      <td>arxiv</td>\n      <td>human</td>\n      <td>direct_prompt</td>\n    </tr>\n    <tr>\n      <th>1402</th>\n      <td>1403</td>\n      <td>An Adaptive Strategy for the Classification of...</td>\n      <td>Claude-instant</td>\n      <td>One of the major problems in computational bio...</td>\n      <td>arxiv</td>\n      <td>human</td>\n      <td>direct_prompt</td>\n    </tr>\n    <tr>\n      <th>1403</th>\n      <td>1404</td>\n      <td>Detailed Models of super-Earths: How well can ...</td>\n      <td>Claude-instant</td>\n      <td>The field of extrasolar planets has rapidly ex...</td>\n      <td>arxiv</td>\n      <td>human</td>\n      <td>direct_prompt</td>\n    </tr>\n    <tr>\n      <th>1404</th>\n      <td>1405</td>\n      <td>The Distribution of AGN in Clusters of Galaxies</td>\n      <td>Claude-instant</td>\n      <td>We present a study of the distribution of AGN ...</td>\n      <td>arxiv</td>\n      <td>human</td>\n      <td>direct_prompt</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_path = f\"{CLEANED_FILES_DIR}arxiv_2800{cleaned_file_version}.parquet\"\n",
    "training_df = pd.read_parquet(training_data_path).reset_index()\n",
    "training_df = prepare_df_for_roberta_training(training_df, \"direct_prompt\")\n",
    "df_claude = training_df[training_df[\"llm_type\"] == \"Claude-instant\"]\n",
    "df_llama, df_palm, df_chatgpt = [training_df[training_df[\"llm_type\"] == _llm].dropna(subset=[\"label\", \"text\"]) for _llm\n",
    "                                 in [\"Llama-2-70b\", \"Google-PaLM\", \"ChatGPT\"]]\n",
    "df_claude.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9314094c5afc0b44",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T08:14:12.507698500Z",
     "start_time": "2025-09-12T08:14:12.499643100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((1120, 7),\n (280, 7),\n 1400,\n 1400,\n 1400,\n Index(['id', 'context', 'llm_type', 'text', 'domain', 'label',\n        'llm_prompting_strategy'],\n       dtype='object'))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df_claude, test_size=0.2, random_state=SEED, shuffle=True)\n",
    "(train_df.shape, test_df.shape, len(df_llama), len(df_claude), len(df_chatgpt),\n",
    " train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3621ef4cc48afee",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T08:23:30.078250400Z",
     "start_time": "2025-09-12T08:14:12.517552800Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6678691506385803, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_runtime': 4.6934, 'eval_samples_per_second': 46.874, 'eval_steps_per_second': 5.966, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5951381921768188, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_runtime': 4.9561, 'eval_samples_per_second': 44.389, 'eval_steps_per_second': 5.65, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5272248983383179, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_runtime': 5.2171, 'eval_samples_per_second': 42.169, 'eval_steps_per_second': 5.367, 'epoch': 3.0}\n",
      "{'train_runtime': 230.0896, 'train_samples_per_second': 11.735, 'train_steps_per_second': 1.473, 'train_loss': 0.6224659607473728, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5272248983383179, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_runtime': 5.0973, 'eval_samples_per_second': 43.16, 'eval_steps_per_second': 5.493, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'b12eac4262bc2189cebaab088a335a4a571334611556d372f58fcaf8350d47c6_train': {'roc_auc': 1.0,\n  'optimal_threshold': -0.501172661781311,\n  'conf_matrix': [[700, 0], [0, 700]],\n  'precision': 1.0,\n  'recall': 1.0,\n  'f1': 1.0,\n  'accuracy': 1.0,\n  'tpr_at_fpr_0_01': 1.0},\n 'eed269866e6fceab0a382db506e57af78c264eb5c02f528bb0ef376bdc071940_train': {'roc_auc': 0.9994599051741908,\n  'optimal_threshold': -0.5214246511459351,\n  'conf_matrix': [[699, 1], [2, 691]],\n  'precision': 0.9985549132947977,\n  'recall': 0.9971139971139971,\n  'f1': 0.9978339350180505,\n  'accuracy': 0.9978463747307968,\n  'tpr_at_fpr_0_01': 0.9956709956709957},\n '95d2a2c3cbfa06bb0cc8b9ba6f93ddc3bb62fd2d4bacb045025746b7135524aa_train': {'roc_auc': 1.0,\n  'optimal_threshold': -0.4787825345993042,\n  'conf_matrix': [[700, 0], [0, 700]],\n  'precision': 1.0,\n  'recall': 1.0,\n  'f1': 1.0,\n  'accuracy': 1.0,\n  'tpr_at_fpr_0_01': 1.0}}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = train_roberta.generate_args_for_training_roberta(\n",
    "    train_df=train_df, test_df=test_df, transfer_df=[df_llama, df_palm, df_chatgpt],\n",
    "    save_model_path=f\"{RESULT_DIR}claude_direct_prompt_test\", device=\"cpu\"\n",
    ")\n",
    "train_roberta.run(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b17314ad2d58b89",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.2 Not Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "248df62379b8f13e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-12T08:33:33.317731800Z",
     "start_time": "2025-09-12T08:23:30.071012700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                            context        llm_type  \\\n",
      "1400  1401  Real Time Turbulent Video Perfecting by Image ...  Claude-instant   \n",
      "1401  1402   Finite Euler products and the Riemann Hypothesis  Claude-instant   \n",
      "1402  1403  An Adaptive Strategy for the Classification of...  Claude-instant   \n",
      "1403  1404  Detailed Models of super-Earths: How well can ...  Claude-instant   \n",
      "1404  1405    The Distribution of AGN in Clusters of Galaxies  Claude-instant   \n",
      "\n",
      "                                                   text domain  label  \\\n",
      "1400  Image and video quality in Long Range Observat...  arxiv  human   \n",
      "1401  We show that if the Riemann Hypothesis is true...  arxiv  human   \n",
      "1402  One of the major problems in computational bio...  arxiv  human   \n",
      "1403  The field of extrasolar planets has rapidly ex...  arxiv  human   \n",
      "1404  We present a study of the distribution of AGN ...  arxiv  human   \n",
      "\n",
      "     llm_prompting_strategy  \n",
      "1400          direct_prompt  \n",
      "1401          direct_prompt  \n",
      "1402          direct_prompt  \n",
      "1403          direct_prompt  \n",
      "1404          direct_prompt  \n",
      "(1120, 7) (280, 7) 1400 1400 1400 Index(['id', 'context', 'llm_type', 'text', 'domain', 'label',\n",
      "       'llm_prompting_strategy'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6690271496772766, 'eval_accuracy': 0.990909090909091, 'eval_f1': 0.990909090909091, 'eval_precision': 0.990909090909091, 'eval_recall': 0.990909090909091, 'eval_runtime': 4.833, 'eval_samples_per_second': 45.52, 'eval_steps_per_second': 5.794, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5879504084587097, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_runtime': 5.3097, 'eval_samples_per_second': 41.433, 'eval_steps_per_second': 5.273, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48871544003486633, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_runtime': 5.5018, 'eval_samples_per_second': 39.987, 'eval_steps_per_second': 5.089, 'epoch': 3.0}\n",
      "{'train_runtime': 238.3346, 'train_samples_per_second': 11.329, 'train_steps_per_second': 1.422, 'train_loss': 0.6202352968289085, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48871544003486633, 'eval_accuracy': 1.0, 'eval_f1': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_runtime': 5.3737, 'eval_samples_per_second': 40.94, 'eval_steps_per_second': 5.211, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/mnt/hdd-baracuda/pdingfelder/mt_philipp_dingfelder_generated_text_detection/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'67b190fbdcf57f867838602f2d91943f81f3ce69f9d6aa273a48ed1401f8944d_train': {'roc_auc': 1.0,\n  'optimal_threshold': -0.501387894153595,\n  'conf_matrix': [[700, 0], [0, 700]],\n  'precision': 1.0,\n  'recall': 1.0,\n  'f1': 1.0,\n  'accuracy': 1.0,\n  'tpr_at_fpr_0_01': 1.0},\n '64dea5d6694c7311e474857861bd1ede726781f69b394924b9e5850bd3e16868_train': {'roc_auc': 0.9987673469387754,\n  'optimal_threshold': -0.5224516987800598,\n  'conf_matrix': [[697, 3], [2, 698]],\n  'precision': 0.9957203994293866,\n  'recall': 0.9971428571428571,\n  'f1': 0.9964311206281228,\n  'accuracy': 0.9964285714285714,\n  'tpr_at_fpr_0_01': 0.9914285714285714},\n '262f2f7c22304d368ea5ae3b6bbe5ef8f1ec041e72221ac21ca3eba27951b9f8_train': {'roc_auc': 1.0,\n  'optimal_threshold': -0.48368963599205017,\n  'conf_matrix': [[700, 0], [0, 700]],\n  'precision': 1.0,\n  'recall': 1.0,\n  'f1': 1.0,\n  'accuracy': 1.0,\n  'tpr_at_fpr_0_01': 1.0}}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = load_dataframe_from_json(json_path_abstract)\n",
    "training_df = prepare_df_for_roberta_training(training_df, \"direct_prompt\")\n",
    "df_claude = training_df[training_df[\"llm_type\"] == \"Claude-instant\"]\n",
    "df_llama, df_palm, df_chatgpt = [training_df[training_df[\"llm_type\"] == _llm].dropna(subset=[\"label\", \"text\"]) for _llm\n",
    "                                 in [\"Llama-2-70b\", \"Google-PaLM\", \"ChatGPT\"]]\n",
    "print(df_claude.head())\n",
    "\n",
    "train_df, test_df = train_test_split(df_claude, test_size=0.2, random_state=SEED, shuffle=True)\n",
    "print(train_df.shape, test_df.shape, len(df_llama), len(df_claude), len(df_chatgpt),\n",
    "      train_df.columns)\n",
    "\n",
    "args = train_roberta.generate_args_for_training_roberta(\n",
    "    train_df=train_df, test_df=test_df, transfer_df=[df_llama, df_palm, df_chatgpt],\n",
    "    save_model_path=f\"{RESULT_DIR}claude_direct_prompt_test\", device=\"cpu\"\n",
    ")\n",
    "\n",
    "train_roberta.run(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
