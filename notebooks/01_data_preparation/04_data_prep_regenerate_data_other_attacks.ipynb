{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Regenerate the data of all used attacks\n",
    "\n",
    "This notebook reprocesses all data augmentations/ attacks, that are not based on 'prompting', i.e. based on the text generation via calls to the llm provider in the first place, but rather use the generated direct prompt and postprocess the generated results.\n",
    "As the original direct_prompt might contain contamination, we use the cleaned data to reprocess these attacks. Some attacks might have minor changes to the original DetectRL code (e.g. by updating to the recent python package or aiming for more efficient and stable generation).\n",
    "Therefore, besides for Dipper, the human generated text was reprocessed as well to ensure identical augmentations for human and llm generated content.\n",
    "Attacks that are based on prompting have already been cleaned in previous steps.\n",
    "\n",
    "Columns to be cleaned here:\n",
    " - adversarial_character_human\n",
    " - adversarial_character_llm\n",
    " - adversarial_word_human\n",
    " - adversarial_word_llm \n",
    " - adversarial_character_word_human\n",
    " - adversarial_character_word_llm\n",
    " - paraphrase_back_translation_human\n",
    " - paraphrase_back_translation_llm\n",
    " - paraphrase_dipper_human (not reprocessed to save computational resources, as the original code was reused one-to-one for dipper)\n",
    " - paraphrase_dipper_llm\n",
    " \n",
    "Columns already cleaned:\n",
    " - direct_prompt\n",
    " - paraphrase_polish_human\n",
    " - paraphrase_polish_llm\n",
    " - prompt_few_shot\n",
    " - prompt_SICO\n",
    "\n",
    "The script is organized as follows:\n",
    "1. Textattack perturbations (character, word and sentence level perturbations)\n",
    "2. Paraphrase back translation (translates text to german and back to english)\n",
    "3. Dipper (well known pre-trained Language Model for paraphrasing on word level, i.e. replace some words with synonyms)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c849dd04ff64e9e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eaa81091f82192d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f78d63250e37ed9"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/pdingfelder/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from warnings import filterwarnings\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sys\n",
    "from typing import Literal\n",
    "from google.cloud import translate\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_DIR = \"../../\"\n",
    "sys.path.append(BASE_DIR)\n",
    "DETECT_RL_DIR = \"../../DetectRL/Data_Generation/\"\n",
    "sys.path.append(DETECT_RL_DIR)\n",
    "sys.path.append(os.path.join(BASE_DIR, \"datasets\"))\n",
    "from src.config import *\n",
    "\n",
    "\n",
    "# textattack for adversarial attacks;\n",
    "#   for sentence based attacks either CLAREAugmenter could be used or TextBuggerAugmenter\n",
    "# from textattack.augmentation.recipes import CLAREAugmenter\n",
    "from textattack.augmentation import CharSwapAugmenter\n",
    "from textattack.augmentation import EmbeddingAugmenter\n",
    "from src.TextAttackTextBugger.text_bugger import TextBuggerAugmenter\n",
    "\n",
    "from src.general_functions_and_patterns_for_detection import (\n",
    "    get_info_based_on_input_path,\n",
    "    CLEANED_FILES_DIR, RESULT_DIR, RECLEANED_FILES_DIR\n",
    ")\n",
    "\n",
    "from DetectRL.Data_Generation.DIPPER import DipperParaphraser, spilt_paragraph"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-18T11:13:42.288093300Z",
     "start_time": "2025-09-18T11:13:42.273085600Z"
    }
   },
   "id": "fca3b302de13a407"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "PRINT_RESULTS = False\n",
    "PRINTING = 0\n",
    "INTERMEDIATE_RESULTS = f\"{RESULT_DIR}/intermediate_results/\"\n",
    "\n",
    "# dipper\n",
    "DIPPER_INTERMEDIATE_DIR = Path(f\"{INTERMEDIATE_RESULTS}/dipper\")\n",
    "os.makedirs(DIPPER_INTERMEDIATE_DIR, exist_ok=True)\n",
    "\n",
    "# paraphrase back translations\n",
    "OUTPUT_DIR = Path(f\"{INTERMEDIATE_RESULTS}/translations\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TRANSLATIONS_DIR = Path(f\"{BASE_DIR}/results/translations\")\n",
    "TRANSLATIONS_DIR.mkdir(parents=True, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-18T11:12:40.747585200Z",
     "start_time": "2025-09-18T11:12:39.892066Z"
    }
   },
   "id": "f5bbcacaf6094592"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Read files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e8acf5abf393ce6"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-18T11:13:47.441124700Z",
     "start_time": "2025-09-18T11:13:45.750832900Z"
    }
   },
   "outputs": [],
   "source": [
    "path_writing = f'{RECLEANED_FILES_DIR}/writing_prompt_2800_recleaned.parquet'\n",
    "path_abstract = f'{RECLEANED_FILES_DIR}/arxiv_2800_recleaned.parquet'\n",
    "path_review = f'{RECLEANED_FILES_DIR}/yelp_review_2800_recleaned.parquet'\n",
    "path_xsum = f'{RECLEANED_FILES_DIR}/xsum_2800_recleaned.parquet'\n",
    "\n",
    "df_writing_cleaned = pd.read_parquet(path_writing)\n",
    "df_abstract_cleaned = pd.read_parquet(path_abstract).drop(columns=[\"human_length\", \"direct_prompt_length\"])\n",
    "df_review_cleaned = pd.read_parquet(path_review)\n",
    "df_xsum_cleaned = pd.read_parquet(path_xsum)\n",
    "\n",
    "domain_dfs = {\n",
    "    \"arxiv\": df_abstract_cleaned.copy(deep=True),\n",
    "    \"writing_prompt\": df_writing_cleaned.copy(deep=True),\n",
    "    \"yelp_review\": df_review_cleaned.copy(deep=True),\n",
    "    \"xsum\": df_xsum_cleaned.copy(deep=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2800 entries, 1 to 2800\n",
      "Data columns (total 20 columns):\n",
      " #   Column                             Non-Null Count  Dtype \n",
      "---  ------                             --------------  ----- \n",
      " 0   id                                 2800 non-null   int64 \n",
      " 1   title                              2800 non-null   object\n",
      " 2   abstract                           2800 non-null   object\n",
      " 3   direct_prompt                      2796 non-null   object\n",
      " 4   llm_type                           2800 non-null   object\n",
      " 5   domain                             2800 non-null   object\n",
      " 6   prompt_few_shot                    2793 non-null   object\n",
      " 7   prompt_SICO                        2798 non-null   object\n",
      " 8   paraphrase_polish_human            2771 non-null   object\n",
      " 9   paraphrase_polish_llm              2796 non-null   object\n",
      " 10  adversarial_character_human        2800 non-null   object\n",
      " 11  adversarial_character_llm          2800 non-null   object\n",
      " 12  adversarial_word_human             2800 non-null   object\n",
      " 13  adversarial_word_llm               2800 non-null   object\n",
      " 14  adversarial_character_word_human   2800 non-null   object\n",
      " 15  adversarial_character_word_llm     2800 non-null   object\n",
      " 16  paraphrase_back_translation_human  2800 non-null   object\n",
      " 17  paraphrase_back_translation_llm    2800 non-null   object\n",
      " 18  paraphrase_dipper_human            2800 non-null   object\n",
      " 19  paraphrase_dipper_llm              2798 non-null   object\n",
      "dtypes: int64(1), object(19)\n",
      "memory usage: 459.4+ KB\n"
     ]
    }
   ],
   "source": [
    "domain_dfs[\"arxiv\"].info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-18T11:13:48.980013600Z",
     "start_time": "2025-09-18T11:13:48.955136800Z"
    }
   },
   "id": "4c4f20360a31aa78"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Text-attacks\n",
    "\n",
    "- implements character, word and sentence level perturbations for human and llm generated text\n",
    "- compared to DetectRL character and word level perturbations have been updated to the most recent textattack package\n",
    "- as the \"TextBuggerAugmenter\" is not present in the version 3.1 of textattack as a plain data augmentation method, the code used in DetectRL has been copied to ensure the same behavior like in the original dataset. CLAREAugmenter could be used as a recent alternative for sentence level perturbations.\n",
    "- both human and llm generated text is re-processed to ensure identical data augmentations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f68d9feab4e225e0"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class DataGenerationAugmentation:\n",
    "    \"\"\"\n",
    "    A class to augment text data in a DataFrame using character, word,\n",
    "    and sentence-level perturbation attacks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_clare: str = \"distilroberta-base\", tokenizer_clare: str = \"distilroberta-base\"):\n",
    "        \"\"\"\n",
    "        Initializes the augmenters for different attack types.\n",
    "\n",
    "        Args:\n",
    "            model_clare (str): Model name or path for the CLARE sentence augmenter.\n",
    "            tokenizer_clare (str): Tokenizer name or path for the CLARE sentence augmenter.\n",
    "        \"\"\"\n",
    "        # A list of available attack strategies\n",
    "        self.attacks = [\"perturbation_character\", \"perturbation_word\", \"perturbation_sent\"]\n",
    "        \n",
    "        # Initialize augmenters from external library textattack\n",
    "        self.word_augmenter = EmbeddingAugmenter(transformations_per_example=1) \n",
    "        self.character_augmenter = CharSwapAugmenter(transformations_per_example=1)\n",
    "        # self.sentence_augmenter = CLAREAugmenter(model=model_clare, tokenizer=tokenizer_clare, transformations_per_example=1)\n",
    "        self.sentence_augmenter = TextBuggerAugmenter(transformations_per_example=1)\n",
    "        \n",
    "    def _get_augmenter(self, attack: str):\n",
    "        \"\"\"A simple factory method to retrieve the correct augmenter based on the attack type.\"\"\"\n",
    "        if attack == \"perturbation_character\":\n",
    "            return self.character_augmenter\n",
    "        elif attack == \"perturbation_word\":\n",
    "            return self.word_augmenter\n",
    "        elif attack == \"perturbation_sent\":\n",
    "            return self.sentence_augmenter\n",
    "        else:\n",
    "            raise ValueError(f\"{attack} is not in perturbation_attacks\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def split_text_in_sentences(input_text: str) -> list:\n",
    "        \"\"\"Splits a block of text into a list of sentences using NLTK.\"\"\"\n",
    "        return nltk.sent_tokenize(input_text)\n",
    "        \n",
    "    def augment_text(self, input_text: str, \n",
    "                     attack: Literal[\"perturbation_character\", \"perturbation_word\", \"perturbation_sent\"] = \"perturbation_character\") -> str:\n",
    "        \"\"\"\n",
    "        Applies a specified augmentation attack to each sentence in the input text.\n",
    "\n",
    "        Args:\n",
    "            input_text (str): The text to augment.\n",
    "            attack (Literal): The type of attack to perform.\n",
    "\n",
    "        Returns:\n",
    "            str: The fully augmented text with sentences rejoined.\n",
    "        \"\"\"\n",
    "        if input_text is None:\n",
    "            return \"\"\n",
    "        if attack not in [\"perturbation_character\", \"perturbation_word\", \"perturbation_sent\"]:\n",
    "            raise ValueError('Attack has to be \"perturbation_character\", \"perturbation_word\" or \"perturbation_sent\"')\n",
    "        # First, split the text into sentences to apply augmentation sentence-by-sentence\n",
    "        split_text = self.split_text_in_sentences(input_text)\n",
    "        augmenter = self._get_augmenter(attack)\n",
    "        \n",
    "        modified_sentences = augmenter.augment_many(split_text)\n",
    "        modified_sentences = [item[0] for item in modified_sentences]\n",
    "        # Rejoin the modified sentences into a single string\n",
    "        output_text: str = ' '.join(modified_sentences)\n",
    "        return output_text\n",
    "    \n",
    "    def execute_perturbation_attacks(self, row: pd.Series, columns_to_use: str|list, attacks: list = None,\n",
    "                                     output_column_suffix: str = \"\"):\n",
    "        \"\"\"\n",
    "        Takes a DataFrame row (as a Series) and applies multiple attacks to multiple columns.\n",
    "\n",
    "        Args:\n",
    "            row (pd.Series): A single row from a DataFrame.\n",
    "            columns_to_use (str | list): The name of the column(s) to augment.\n",
    "            attacks (list, optional): A list of attacks to apply. Defaults to all available attacks.\n",
    "            output_column_suffix (str, optional): A suffix to add to the generated column names.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary where keys are the new column names and values are the augmented texts.\n",
    "        \"\"\"\n",
    "        # Use all default attacks if none are specified\n",
    "        attacks = self.attacks if attacks is None else attacks\n",
    "        # Ensure 'columns_to_use' is always a list for consistent processing\n",
    "        columns_to_use = [columns_to_use] if isinstance(columns_to_use, str) else columns_to_use\n",
    "        \n",
    "        # The output dictionary starts with the row's ID for potential merging later\n",
    "        output = {\"id\": row[\"id\"]}\n",
    "        \n",
    "        # Nested loops to apply each attack to each specified column\n",
    "        for _column_to_attack in columns_to_use:\n",
    "            for _attack in attacks:\n",
    "                # Create a dynamic key for the output dictionary, e.g., \"perturbation_word_direct_prompt\"\n",
    "                new_col_name = f\"{_attack}_{_column_to_attack}{output_column_suffix}\"\n",
    "                try:\n",
    "                    output[new_col_name] = self.augment_text(row[_column_to_attack], attack=_attack)\n",
    "                except Exception as e:\n",
    "                    raise e\n",
    "                    # print(e)\n",
    "                    # output[new_col_name] = \"ERROR\"\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-13T10:09:15.222908200Z",
     "start_time": "2025-08-13T10:09:15.197791300Z"
    }
   },
   "id": "eb37bd6ca173e4f2"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows for arxiv:  88%|████████▊ | 2475/2800 [2:45:24<17:49,  3.29s/it]   Building prefix dict from the default dictionary ...\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.714 seconds.\n",
      "Loading model cost 0.714 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Processing rows for arxiv: 100%|██████████| 2800/2800 [3:05:34<00:00,  3.98s/it]  \n",
      "Processing rows for writing_prompt: 100%|██████████| 2800/2800 [5:57:52<00:00,  7.67s/it]   \n",
      "Processing rows for yelp_review: 100%|██████████| 2800/2800 [3:34:50<00:00,  4.60s/it]   \n",
      "Processing rows for xsum: 100%|██████████| 2800/2800 [13:18:43<00:00, 17.12s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Instantiate the class\n",
    "augmenter = DataGenerationAugmentation()\n",
    "\n",
    "for _domain, df in domain_dfs.items():\n",
    "    # Extract the needed key for this domain\n",
    "    _, _, human_key = get_info_based_on_input_path(_domain)\n",
    "    \n",
    "    \n",
    "    # Make sure to set the tqdm description dynamically\n",
    "    tqdm.pandas(desc=f\"Processing rows for {_domain}\")\n",
    "\n",
    "    # Progress bar per row with domain as description\n",
    "    results_series = df.progress_apply(\n",
    "        lambda row: augmenter.execute_perturbation_attacks(\n",
    "            row,\n",
    "            columns_to_use=[\"direct_prompt\", human_key],\n",
    "            \n",
    "            # due to processing times sentence perturbations have been executed separately\n",
    "            # attacks=[\"perturbation_character\", \"perturbation_word\"]\n",
    "            attacks=[\"perturbation_sent\"]\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Convert the resulting series of dictionaries into a new DataFrame\n",
    "    generated_df = pd.DataFrame(results_series.to_list())\n",
    "    generated_df.to_parquet(\n",
    "        f\"{INTERMEDIATE_RESULTS}/{_domain}_text_attacks_sentences.parquet\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-14T12:10:50.736581400Z",
     "start_time": "2025-08-13T10:13:49.504041Z"
    }
   },
   "id": "6fd20cbc9deeaf3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Paraphrase back translation\n",
    "\n",
    "- translates the text to german and back to english again\n",
    "- 'en-US' was chosen, as the API does not allow solely 'en' anymore\n",
    "- updated the code to use the google TranslationServiceClient-API instead of Google Translate. To re-execute this part it is required to have a TranslationService within the google workspace enabled as well as a authentication method. \n",
    "- The data is split in chunks of 10 (or 15 for arxiv) human or llm entries to avoid token limitations of the translation API\n",
    "- Due to minor changes in the translation setup both human and llm generated text is re-translated."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e531972bdfda99f5"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def translate_text(text: list, project_id=\"linen-arch-469021-f5\", target_language_code: str = \"de\",\n",
    "                   source_language_code: str = \"en-US\"):\n",
    "    \"\"\"\n",
    "    Translates text using a service account for authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    # The client automatically finds and uses the credentials from the\n",
    "    # GOOGLE_APPLICATION_CREDENTIALS environment variable.\n",
    "    client = translate.TranslationServiceClient()\n",
    "\n",
    "    location = \"global\"\n",
    "    parent = f\"projects/{project_id}/locations/{location}\"\n",
    "    \n",
    "    response = client.translate_text(\n",
    "        request={\n",
    "            \"parent\": parent,\n",
    "            \"contents\": text,\n",
    "            \"mime_type\": \"text/plain\",\n",
    "            \"source_language_code\": source_language_code,\n",
    "            \"target_language_code\": target_language_code,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    response = client.translate_text(\n",
    "        request={\n",
    "            \"parent\": parent,\n",
    "            \"contents\": [item.translated_text for item in response.translations],\n",
    "            \"mime_type\": \"text/plain\",\n",
    "            \"source_language_code\": target_language_code,\n",
    "            \"target_language_code\": source_language_code,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Return a list of the translated text strings\n",
    "    return [item.translated_text for item in response.translations]\n",
    "\n",
    "\n",
    "def process_and_translate_dataframes(domain_dfs: dict, documents_to_translate: int = 10):\n",
    "    \"\"\"\n",
    "    Iterates through a dictionary of dataframes, translates specified columns\n",
    "    in batches, and saves the results as pickle files.\n",
    "    \"\"\"\n",
    "    # Iterate over each domain and its corresponding dataframe\n",
    "    for domain, df in domain_dfs.items():\n",
    "        print(f\"--- Processing domain: {domain} ---\")\n",
    "        _, _, human_column = get_info_based_on_input_path(domain)\n",
    "        # Process the dataframe in chunks of 100 rows\n",
    "        for i in tqdm(range(0, len(df), documents_to_translate)):\n",
    "            batch_df = df.iloc[i:i+documents_to_translate]\n",
    "            batch_counter = i // documents_to_translate\n",
    "            \n",
    "            # print(f\"  Translating batch {batch_counter} (rows {i} to {i+documents_to_translate-1})...\")\n",
    "            \n",
    "            # Define the output file path\n",
    "            filename = f\"{domain}_translate_{batch_counter}.pkl\"\n",
    "            output_path = OUTPUT_DIR / filename\n",
    "            \n",
    "            if filename not in os.listdir(OUTPUT_DIR):\n",
    "                \n",
    "                # Extract text from the two columns to be translated\n",
    "                prompts_to_translate = batch_df['direct_prompt'].tolist()\n",
    "                prompts_to_translate = [\" \" if item is None else item for item in prompts_to_translate]\n",
    "                # prompts_to_translate\n",
    "                humans_to_translate = batch_df[human_column].tolist()\n",
    "                humans_to_translate = [\" \" if item is None else item for item in humans_to_translate]\n",
    "                # print(prompts_to_translate, humans_to_translate)\n",
    "                # Call the translation API for each list of texts\n",
    "                translated_prompts = translate_text(text=prompts_to_translate)\n",
    "                translated_humans = translate_text(text=humans_to_translate)\n",
    "                \n",
    "                # Store results in a new dataframe\n",
    "                result_df = pd.DataFrame({\n",
    "                    'translated_direct_prompt': translated_prompts,\n",
    "                    'translated_human': translated_humans\n",
    "                })\n",
    "                # Keep the original index to align with the source data\n",
    "                result_df.index = batch_df.index\n",
    "                output_path = str(output_path).replace(\".pkl\", \"_v2.pkl\")\n",
    "                # Save the resulting dataframe to a pickle file\n",
    "                result_df.to_pickle(output_path)\n",
    "                \n",
    "            else:\n",
    "                result_df = pd.read_pickle(output_path)\n",
    "                # Extract text from the two columns to be translated\n",
    "                prompts_to_translate = batch_df['direct_prompt'].tolist()\n",
    "                prompts_to_translate = [\" \" if item is None else item for item in prompts_to_translate]\n",
    "\n",
    "                # Call the translation API for each list of texts\n",
    "                translated_prompts = translate_text(text=prompts_to_translate)\n",
    "                \n",
    "                # Store results in a new dataframe\n",
    "                result_df['translated_direct_prompt'] = translated_prompts\n",
    "                # Keep the original index to align with the source data\n",
    "                result_df.index = batch_df.index\n",
    "                \n",
    "                output_path = str(output_path).replace(\".pkl\", \"_v2.pkl\")\n",
    "                # print(output_path)\n",
    "                # Save the resulting dataframe to a pickle file\n",
    "                result_df.to_pickle(output_path)\n",
    "            \n",
    "def combine_translated_text_files(domain_name: str, store_dir: str = OUTPUT_DIR, start_counter: int = 0, end_counter: int = 280,\n",
    "                                  prefix: str = \"_v2\"):\n",
    "    list_all_translations = []\n",
    "    for _counter in range(start_counter, end_counter):\n",
    "        output_path = f\"{store_dir}/{domain_name}_translate_{_counter}.pkl\"\n",
    "        if prefix:\n",
    "            output_path = str(output_path).replace(\".pkl\", \"_v2.pkl\")\n",
    "        translated_content = pd.read_pickle(output_path)\n",
    "        list_all_translations.append(translated_content)\n",
    "    return list_all_translations\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-15T07:24:23.970745800Z",
     "start_time": "2025-08-15T07:24:23.937250100Z"
    }
   },
   "id": "96945e682dfb4e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "process_and_translate_dataframes(domain_dfs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a23324987cdb7884"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# combine intermediate files to one DataFrame per domain\n",
    "for _domain in domain_dfs.keys():\n",
    "    end_counter = 280   #  187 if _domain == \"arxiv\" else\n",
    "    results_translated = pd.concat(combine_translated_text_files(_domain, end_counter=end_counter, prefix=\"_v2\"))\n",
    "    results_translated.to_parquet(f\"{TRANSLATIONS_DIR}/{_domain}_translated_files.parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-15T07:24:39.793483600Z",
     "start_time": "2025-08-15T07:24:37.378058300Z"
    }
   },
   "id": "772ef9a7624f2c9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Dipper"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4ac6b734175ff89"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "855f113658f643568dbf387d94154275"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kalpeshk2011/dipper-paraphraser-xxl model loaded in 1.5297589302062988\n",
      "arxiv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53it [04:56,  4.42s/it]"
     ]
    }
   ],
   "source": [
    "dp = DipperParaphraser(model=\"kalpeshk2011/dipper-paraphraser-xxl\")\n",
    "\n",
    "for _domain, df in domain_dfs.items():\n",
    "    print(_domain)\n",
    "    _, _, human_key = get_info_based_on_input_path(_domain)\n",
    "\n",
    "    data = df.copy(deep=True)\n",
    "    \n",
    "    human_error_list = []\n",
    "    llm_error_list = []\n",
    "    dipper_results = data.copy(deep=True)\n",
    "    dipper_results[\"paraphrase_dipper_llm_new\"] = None\n",
    "    counter = 0\n",
    "    last_stored = 0\n",
    "\n",
    "    for index, article in tqdm(data.iterrows()):\n",
    "\n",
    "        abstract = article[human_key]\n",
    "        direct_prompt = article['direct_prompt']\n",
    "        \n",
    "        # dipper for human is not repeated, as we use the same model and parameters\n",
    "        try:\n",
    "            # if not (abstract is None or abstract == \"\"):\n",
    "            #     prompt, input_text = spilt_paragraph(abstract)\n",
    "            #     if len(input_text) >= 1024:\n",
    "            #         input_text = input_text[:1024]\n",
    "            #         input_text2 = input_text[1024:]\n",
    "            #     else: input_text2 = None\n",
    "            #     paraphrase_dipper_human = dp.paraphrase(input_text, lex_diversity=40, order_diversity=40, prefix=prompt,\n",
    "            #                                             do_sample=False, max_length=1024)\n",
    "            #     if input_text2 is not None:\n",
    "            #         paraphrase_dipper_human += dp.paraphrase(input_text2, lex_diversity=40, order_diversity=40, prefix=prompt,\n",
    "            #                                             do_sample=False, max_length=1024)\n",
    "            #     if len(paraphrase_dipper_human) == 0:\n",
    "            #         human_error_list.append(article)\n",
    "            #         \n",
    "            #     article['paraphrase_dipper_human'] = paraphrase_dipper_human\n",
    "            # else:\n",
    "            #     article['paraphrase_dipper_human'] = None\n",
    "            #     # time.sleep(random.randint(1, 4))\n",
    "    \n",
    "            if not (direct_prompt is None or direct_prompt == \"\"):\n",
    "                prompt, input_text = spilt_paragraph(direct_prompt)\n",
    "                # if len(input_text) >= 1024:\n",
    "                #     input_text = input_text[:1024]\n",
    "                #     input_text2 = input_text[1024:]\n",
    "                # else: input_text2 = None\n",
    "                \n",
    "                paraphrase_dipper_llm = dp.paraphrase(input_text, lex_diversity=40, order_diversity=40, prefix=prompt,\n",
    "                                                        do_sample=False, max_length=2048)\n",
    "                # if input_text2 is not None:\n",
    "                #     paraphrase_dipper_llm += dp.paraphrase(input_text2, lex_diversity=40, order_diversity=40, prefix=prompt,\n",
    "                #                                         do_sample=False, max_length=1024)\n",
    "                    \n",
    "            else:\n",
    "                paraphrase_dipper_llm = None\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            paraphrase_dipper_llm, paraphrase_dipper_human = \"\", \"\"\n",
    "                \n",
    "        if paraphrase_dipper_llm is None:\n",
    "            llm_error_list.append(article)\n",
    "        elif len(paraphrase_dipper_llm) == 0:\n",
    "            llm_error_list.append(article)\n",
    "        dipper_results.loc[index, \"paraphrase_dipper_llm_new\"] = paraphrase_dipper_llm\n",
    "        \n",
    "        counter += 1\n",
    "        if counter % 100 == 0:\n",
    "            dipper_results.iloc[last_stored:counter].to_parquet(f\"{DIPPER_INTERMEDIATE_DIR}/dipper_temp_{_domain}_{counter}.parquet\")\n",
    "            last_stored = counter\n",
    "    \n",
    "    dipper_results.to_parquet(f\"{DIPPER_INTERMEDIATE_DIR}/{_domain}_dipper_final_test_run.parquet\")\n",
    "\n",
    "    print(f\"human error number:{len(human_error_list)}\")\n",
    "    print(f\"llm error number:{len(llm_error_list)}\")\n",
    "\n",
    "    # Convert list of Series -> DataFrame\n",
    "    human_error_df = pd.DataFrame(human_error_list)\n",
    "    llm_error_df   = pd.DataFrame(llm_error_list)\n",
    "    # save as CSV\n",
    "    human_error_df.to_parquet(f\"{DIPPER_INTERMEDIATE_DIR}/{_domain}_human_error.parquet\", index=False)\n",
    "    llm_error_df.to_parquet(f\"{DIPPER_INTERMEDIATE_DIR}/{_domain}_llm_error.parquet\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-08-15T16:01:37.513018600Z"
    }
   },
   "id": "bd6ea3fbb183f7dd"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "for _, item in dipper_results.iloc[2100:2103].iterrows():\n",
    "    print(item.paraphrase_dipper_llm, \"\\n\")\n",
    "    print(item.paraphrase_dipper_llm_new, \"\\n\")\n",
    "    print(item.direct_prompt, \"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-15T10:43:22.894404400Z",
     "start_time": "2025-08-15T10:43:22.891402100Z"
    }
   },
   "id": "e1719ded9cb52df1"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'The menu was a riot of mouth-watering choices, and we were spoiled for choice. We started our culinary journey with the chef’s special appetizer, a fusion of flavors that danced on our tongues. For the main course, we chose their signature dish, and it was beyond our expectations. The chicken was tender and juicy, and the accompanying sauce was tangy and delicious. The presentation alone was a work of art, and showed the care and attention that went into each dish. The portions were generous, and we left the table feeling completely satisfied. What really stood out was the impeccable service we received throughout our meal. The staff was attentive and knowledgeable, and they were able to accommodate our dietary restrictions. It was obvious that they took great pride in making sure that each guest had a memorable experience. To top it all off, the desserts were divine. We indulged in a rich chocolate cake that melted in our mouths, accompanied by a velvety scoop of homemade vanilla ice cream. The combination of flavors and textures was pure bliss, and we couldn’t help but order another serving. The ambiance was warm and inviting, with soft lighting and tasteful decor that created a cozy ambience. The background music was subtle and added to the overall charm of the place, allowing for comfortable conversation without overpowering the senses. In short, this place exceeded our expectations in every way. Considering the quality of the food, the impeccable service, and the delightful ambience, the prices were surprisingly reasonable. It’s rare to find a gem that combines exceptional taste with reasonable prices.'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_dfs[\"yelp_review\"].paraphrase_dipper_llm.loc[4]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-15T10:21:04.662352600Z",
     "start_time": "2025-08-15T10:21:04.651051Z"
    }
   },
   "id": "8b2333f14501b7c"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I had the misfortune of becoming a patient at Dr. Goldberg's office recently, and it was an experience that left me utterly disappointed and frustrated. From the moment I stepped into the waiting room, I sensed an air of disorganization and chaos. The receptionist seemed overwhelmed and didn't bother to greet me or even acknowledge my presence for several minutes. When I finally got called in, my initial impression of Dr. Goldberg was far from positive. He was dismissive and seemed disinterested in listening to my concerns. It felt as if he was just rushing through the appointment without giving me the attention or care I deserved. Moreover, his diagnostic skills were questionable at best. He made a hasty diagnosis without even considering other possibilities or conducting the necessary tests. I left the office feeling more confused and unsure about my condition than when I arrived. The lack of follow-up from the doctor or his staff only worsened my frustration. Despite repeated attempts to contact them regarding my ongoing symptoms, I was met with silence and indifference. It's evident that Dr. Goldberg and his office prioritize quantity over quality, with little regard for the well-being of their patients. In conclusion, I strongly advise anyone in need of medical care to avoid this doctor and his office at all costs. There are plenty of competent and compassionate healthcare providers out there who genuinely prioritize their patients' health and well-being. \n",
      "\n",
      "From the moment I stepped into the waiting room, I sensed a chaotic disarray. The receptionist seemed overwhelmed and didn't greet me or even acknowledge my presence for several minutes. When I was finally called in, my first impression of Dr. Goldberg was far from favourable. He was dismissive and seemed disinterested in listening to my concerns. He seemed to be rushing through the appointment without giving me the attention or care I deserved. Moreover, his diagnostic skills were questionable at best. He made a hasty diagnosis without even considering other possibilities or running the necessary tests. I left the office feeling more confused and uncertain about my condition than when I had arrived. The lack of follow-up from the doctor or his staff only added to my frustration. I made repeated attempts to contact them about my ongoing symptoms, but I was met with silence and indifference. In conclusion, I strongly advise anyone in need of medical care to avoid this doctor and his office at all costs. It is clear that Dr. Goldberg and his office place a high priority on quantity over quality, and they have little regard for the well-being of their patients. There are many competent and compassionate health-care professionals who genuinely care about their patients' health and well-being. \n",
      "\n",
      "\n",
      "Let me start by saying that my experience with Dr. Smith was utterly disappointing. From the moment I stepped into the waiting room, I was met with an unprofessional and disorganized atmosphere. The receptionist seemed overwhelmed and was unable to provide me with any clear information regarding my appointment. After a frustratingly long wait, I finally got to see Dr. Smith. However, their demeanor was distant and devoid of any empathy. It felt as though they were rushing through the consultation, barely listening to my concerns. I left the office feeling dismissed and unheard. Furthermore, Dr. Smith's treatment plan was questionable at best. They prescribed medications without thoroughly explaining the potential side effects or alternative options. This lack of transparency made me lose confidence in their medical expertise. On top of that, their follow-up was nonexistent. I had to proactively reach out to their office multiple times to get basic information and test results. It's clear that Dr. Smith lacks both professional competence and basic courtesy towards their patients. I strongly advise seeking care from someone who values patient well-being and respects their time. Save yourself the frustration and disappointment by looking elsewhere for a competent and compassionate healthcare provider. \n",
      "\n",
      "Let me start by saying that my experience with Dr. Smith was a complete disappointment. From the moment I entered the waiting room, I was met with an unprofessional and disorganized atmosphere. The receptionist seemed overwhelmed and was unable to give me any clear information about my appointment. After a long wait, I finally got to see Dr. Smith. However, his attitude was distant and he barely listened to my concerns. Moreover, his treatment plan was questionable at best. He prescribed medications without explaining the potential side effects or alternative treatments. I left the office feeling dismissed and unheard. On top of that, their follow-up was non-existent. I had to call the office several times to get basic information and test results. This lack of transparency made me lose confidence in their medical expertise. I strongly recommend that you seek care elsewhere from a competent and compassionate health care provider. It is clear that Dr. Smith lacks both professional competence and basic courtesy towards his patients. Save yourself the frustration and disappointment and seek care elsewhere from a competent and compassionate health care provider. \n",
      "\n",
      "\n",
      "Owning a driving range inside the city limits is like a license to print money. The convenience and accessibility it offers to urban residents who crave a golfing experience without leaving the city is impeccable. As soon as you step foot onto the well-manicured greens, you are transported to a tranquil oasis, away from the hustle and bustle of city life. The range boasts state-of-the-art facilities that cater to players of all skill levels, from beginners to seasoned professionals.The driving range is equipped with automated ball dispensers, making it incredibly easy to keep the balls coming without any interruptions. The staff is friendly and attentive, always ready to assist and provide helpful tips to improve your swing. Additionally, the range offers private lessons with experienced golf instructors who are passionate about perfecting your technique.The range's amenities go beyond just the driving range itself. There is a cozy clubhouse overlooking the greens, where you can unwind after a satisfying session with fellow golf enthusiasts. The clubhouse offers a variety of refreshments and snacks, ensuring you are well taken care of during your visit. It's a fantastic spot to socialize and make new connections with like-minded individuals who share your love for the game.What sets this driving range apart from others is its commitment to continuous improvement. The management regularly updates the range's equipment and technology, ensuring that players have access to the latest advancements in the golf industry. The range also hosts regular tournaments and events, adding an element of excitement and competition to the overall experience.The driving range's location within the city also offers additional benefits. With easy accessibility via public transportation, it attracts a diverse clientele, including busy professionals and families looking for a fun activity nearby. It serves as a fantastic venue for corporate events and team-building exercises. \n",
      "\n",
      "The range offers state-of-the-art facilities for golfers of all skill levels, from beginners to professionals. The driving range is a convenient and easily accessible place for city dwellers who want to play golf without leaving the city. As soon as you step onto the well-tended greens, you are transported to a peaceful oasis, far from the hustle and bustle of the city. The staff is friendly and attentive, always ready to help and give you useful tips to improve your swing. The range is equipped with automatic ball dispensers, which make it easy to keep the balls coming without interruption. The range also offers private lessons with experienced golf instructors who are passionate about perfecting your technique. The amenities at the range go beyond the driving range itself. There is a cozy clubhouse overlooking the greens, where you can relax after a satisfying session with fellow golfers. The clubhouse offers a variety of refreshments and snacks to keep you well-nourished during your visit. The driving range is distinguished from others by its commitment to continuous improvement. The management is constantly investing in new equipment and technology to keep the range up to date with the latest developments in the golf industry. It is a great place to socialize and make new friends who share your passion for golf. The driving range’s location in the city offers additional benefits. It is easily accessible by public transportation, which makes it a popular destination for busy professionals and families looking for a fun activity close to home. The range also hosts regular tournaments and events, which add an element of excitement and competition to the overall experience. It is an ideal venue for business events and team-building exercises. \n",
      "\n",
      "\n",
      "This place was DELICIOUS!! From the moment we stepped in, the aroma of freshly baked bread and savory spices enveloped us, tantalizing our taste buds. The menu boasted an impressive array of mouthwatering options, leaving us spoilt for choice. We started our culinary journey with the chef's special appetizer, a delectable fusion of flavors that danced on our tongues. The presentation alone was a work of art, showcasing the attention to detail that went into each dish.For the main course, we opted for their signature dish, and it exceeded all expectations. The chicken was perfectly cooked, tender and juicy, while the accompanying sauce added a burst of tanginess that complemented the dish beautifully. The portion sizes were generous, ensuring we left the table feeling completely satisfied.What truly stood out was the impeccable service we received throughout our dining experience. The staff were incredibly attentive and knowledgeable, providing helpful recommendations while catering to any dietary restrictions. It was evident that they took great pride in ensuring every guest had a memorable time.To top it off, the dessert selection was simply divine. We indulged in a rich chocolate cake that melted in our mouths, paired with a velvety scoop of homemade vanilla ice cream. The combination of flavors and textures was pure bliss, leaving us longing for another slice.The ambiance of the restaurant was warm and inviting, with soft lighting and tasteful decor that created a cozy atmosphere. The background music was subtle and added to the overall charm of the place, allowing for comfortable conversation without overpowering the senses.Considering the quality of the food, the impeccable service, and the delightful ambiance, the prices were surprisingly reasonable. It's rare to find such a gem that balances both exceptional taste and affordability.In summary, this place exceeded our expectations in every aspect. \n",
      "\n",
      "The menu was a riot of mouth-watering choices, and we were spoiled for choice. We started our culinary journey with the chef’s special appetizer, a fusion of flavors that danced on our tongues. For the main course, we chose their signature dish, and it was beyond our expectations. The chicken was tender and juicy, and the accompanying sauce was tangy and delicious. The presentation alone was a work of art, and showed the care and attention that went into each dish. The portions were generous, and we left the table feeling completely satisfied. What really stood out was the impeccable service we received throughout our meal. The staff was attentive and knowledgeable, and they were able to accommodate our dietary restrictions. It was obvious that they took great pride in making sure that each guest had a memorable experience. To top it all off, the desserts were divine. We indulged in a rich chocolate cake that melted in our mouths, accompanied by a velvety scoop of homemade vanilla ice cream. The combination of flavors and textures was pure bliss, and we couldn’t help but order another serving. The ambiance was warm and inviting, with soft lighting and tasteful decor that created a cozy ambience. The background music was subtle and added to the overall charm of the place, allowing for comfortable conversation without overpowering the senses. In short, this place exceeded our expectations in every way. Considering the quality of the food, the impeccable service, and the delightful ambience, the prices were surprisingly reasonable. It’s rare to find a gem that combines exceptional taste with reasonable prices. \n"
     ]
    }
   ],
   "source": [
    "for _, item in pd.read_parquet(f\"{DIPPER_INTERMEDIATE_DIR}/yelp_review_dipper_final_test_run.parquet\").iterrows():\n",
    "    print(item[\"direct_prompt\"], \"\\n\")\n",
    "    print(item[\"paraphrase_dipper_llm\"], \"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-15T10:11:50.042641100Z",
     "start_time": "2025-08-15T10:11:49.995800Z"
    }
   },
   "id": "1c67e3b9bbd023ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Store the results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cec8a3841005c2ed"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# 1. read the dataframes of the different attacks\n",
    "# 2. replace the original attack based on the contaminated text with the regenerated one\n",
    "# 3. save the dataframes in the dict again\n",
    "\n",
    "for _domain, original_df in domain_dfs.items():\n",
    "    dipper_df: pd.DataFrame = pd.read_parquet(f\"{DIPPER_INTERMEDIATE_DIR}/{_domain}_dipper_final.parquet\")\n",
    "    text_attack_sentences_df: pd.DataFrame = pd.read_parquet(f\"{INTERMEDIATE_RESULTS}/{_domain}_text_attacks_sentences.parquet\")\n",
    "    text_attack_word_and_character_df: pd.DataFrame = pd.read_parquet(f\"{INTERMEDIATE_RESULTS}/{_domain}_text_attacks.parquet\")\n",
    "    paraphrase_back_translation_df: pd.DataFrame = pd.read_parquet(f\"{TRANSLATIONS_DIR}/{_domain}_translated_files.parquet\")\n",
    "    dfs = [dipper_df, text_attack_sentences_df, text_attack_word_and_character_df, paraphrase_back_translation_df]\n",
    "    dataframe_with_all_attacks = dipper_df.copy(deep=True)\n",
    "\n",
    "    for _df in dfs:\n",
    "        # replace all empty phrases with None\n",
    "        #   the number of Nones should be the same for all dataframes for the human/ llm generated text\n",
    "        #   --> if something has a None (e.g. due to rejections) for direct prompt, the other attacks have to be empty,\n",
    "        #   if not, the other attacks must have a value\n",
    "        _df.replace([\"ERROR\", \"\", \" \"], None, inplace=True)\n",
    "    \n",
    "    # Same for paraphrase_back_translation_df\n",
    "    paraphrase_back_translation_df = paraphrase_back_translation_df.reset_index()\n",
    "    paraphrase_back_translation_df.columns = ['id' if col == paraphrase_back_translation_df.columns[0] else col \n",
    "                                              for col in paraphrase_back_translation_df.columns]\n",
    "    \n",
    "    # Ensure 'id' is a column in dipper_df without index-name clash\n",
    "    dipper_df = dipper_df.reset_index(drop=True)\n",
    "    dipper_df.drop(columns=[\"adversarial_character_human\", \"adversarial_character_llm\", \"adversarial_word_human\", \"adversarial_word_llm\",\n",
    "                            \"adversarial_character_word_human\", \"adversarial_character_word_llm\", \n",
    "                            \"paraphrase_back_translation_human\", \"paraphrase_back_translation_llm\",\n",
    "                            \"paraphrase_dipper_llm\"\n",
    "                    ], inplace=True)\n",
    "    \n",
    "    # Merge with dipper_df as base, keeping only dipper_df's rows\n",
    "    merged_df: pd.DataFrame = (\n",
    "        dipper_df\n",
    "        .merge(text_attack_sentences_df, on=\"id\", how=\"left\")\n",
    "        .merge(text_attack_word_and_character_df, on=\"id\", how=\"left\")\n",
    "        .merge(paraphrase_back_translation_df, on=\"id\", how=\"left\")\n",
    "    )\n",
    "      \n",
    "    _, _, human_key = get_info_based_on_input_path(_domain)\n",
    "    merged_df.rename(columns={f\"perturbation_character_{human_key}\": \"adversarial_character_human\", \n",
    "                      \"perturbation_character_direct_prompt\": \"adversarial_character_llm\", \n",
    "                      f\"perturbation_word_{human_key}\": \"adversarial_word_human\", \n",
    "                      \"perturbation_word_direct_prompt\": \"adversarial_word_llm\",\n",
    "                      f\"perturbation_sent_{human_key}\": \"adversarial_character_word_human\", \n",
    "                      \"perturbation_sent_direct_prompt\": \"adversarial_character_word_llm\", \n",
    "                      \"translated_human\": \"paraphrase_back_translation_human\", \n",
    "                      \"translated_direct_prompt\": \"paraphrase_back_translation_llm\",\n",
    "                      \"paraphrase_dipper_llm_new\": \"paraphrase_dipper_llm\"\n",
    "                    }, inplace=True)\n",
    "    \n",
    "    # Restore original index from 'id'\n",
    "    merged_df = merged_df.set_index('id')\n",
    "    # Get column order from original dataframe, dropping 'id'\n",
    "    target_cols = [col for col in original_df.columns if col != \"id\"]\n",
    "    # Keep only columns that exist in merged_df\n",
    "    target_cols = [col for col in target_cols if col in merged_df.columns]\n",
    "    # Reorder merged_df\n",
    "    merged_df = merged_df[target_cols]\n",
    "    \n",
    "    # restore the dataframe in the dict of all dataframes\n",
    "    domain_dfs[_domain] = merged_df.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-15T09:16:13.771453Z",
     "start_time": "2025-08-15T09:16:12.161314300Z"
    }
   },
   "id": "2951839b0bb758fd"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "path_writing = f'{CLEANED_FILES_DIR}/writing_prompt_2800_cleaned_final.parquet'\n",
    "path_abstract = f'{CLEANED_FILES_DIR}/arxiv_2800_cleaned_final.parquet'\n",
    "path_review = f'{CLEANED_FILES_DIR}/yelp_review_2800_cleaned_final.parquet'\n",
    "path_xsum = f'{CLEANED_FILES_DIR}/xsum_2800_cleaned_final.parquet'\n",
    "\n",
    "domain_dfs[\"writing_prompt\"].to_parquet(path_writing)\n",
    "domain_dfs[\"arxiv\"].to_parquet(path_abstract)\n",
    "domain_dfs[\"yelp_review\"].to_parquet(path_review)\n",
    "domain_dfs[\"xsum\"].to_parquet(path_xsum)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-15T09:16:27.392023500Z",
     "start_time": "2025-08-15T09:16:25.897677600Z"
    }
   },
   "id": "3e432ef21c423107"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2800 entries, 1 to 2800\n",
      "Data columns (total 19 columns):\n",
      " #   Column                             Non-Null Count  Dtype \n",
      "---  ------                             --------------  ----- \n",
      " 0   title                              2800 non-null   object\n",
      " 1   abstract                           2800 non-null   object\n",
      " 2   direct_prompt                      2793 non-null   object\n",
      " 3   llm_type                           2800 non-null   object\n",
      " 4   domain                             2800 non-null   object\n",
      " 5   prompt_few_shot                    2793 non-null   object\n",
      " 6   prompt_SICO                        2798 non-null   object\n",
      " 7   paraphrase_polish_human            2771 non-null   object\n",
      " 8   paraphrase_polish_llm              2795 non-null   object\n",
      " 9   adversarial_character_human        2800 non-null   object\n",
      " 10  adversarial_character_llm          2793 non-null   object\n",
      " 11  adversarial_word_human             2800 non-null   object\n",
      " 12  adversarial_word_llm               2793 non-null   object\n",
      " 13  adversarial_character_word_human   2800 non-null   object\n",
      " 14  adversarial_character_word_llm     2793 non-null   object\n",
      " 15  paraphrase_back_translation_human  2798 non-null   object\n",
      " 16  paraphrase_back_translation_llm    2791 non-null   object\n",
      " 17  paraphrase_dipper_human            2800 non-null   object\n",
      " 18  paraphrase_dipper_llm              2798 non-null   object\n",
      "dtypes: object(19)\n",
      "memory usage: 502.0+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2800 entries, 1 to 2800\n",
      "Data columns (total 20 columns):\n",
      " #   Column                             Non-Null Count  Dtype \n",
      "---  ------                             --------------  ----- \n",
      " 0   story                              2800 non-null   object\n",
      " 1   story_prompt                       2800 non-null   object\n",
      " 2   direct_prompt                      2749 non-null   object\n",
      " 3   llm_type                           2800 non-null   object\n",
      " 4   domain                             2800 non-null   object\n",
      " 5   paraphrase_polish_human            2538 non-null   object\n",
      " 6   paraphrase_polish_llm              2755 non-null   object\n",
      " 7   prompt_few_shot                    2325 non-null   object\n",
      " 8   prompt_SICO                        2743 non-null   object\n",
      " 9   adversarial_character_human        2800 non-null   object\n",
      " 10  adversarial_character_llm          2749 non-null   object\n",
      " 11  adversarial_word_human             2800 non-null   object\n",
      " 12  adversarial_word_llm               2749 non-null   object\n",
      " 13  adversarial_character_word_human   2800 non-null   object\n",
      " 14  adversarial_character_word_llm     2749 non-null   object\n",
      " 15  paraphrase_back_translation_human  2800 non-null   object\n",
      " 16  paraphrase_back_translation_llm    2749 non-null   object\n",
      " 17  paraphrase_dipper_human            2793 non-null   object\n",
      " 18  paraphrase_dipper_llm              2793 non-null   object\n",
      " 19  icl_prompt                         2 non-null      object\n",
      "dtypes: object(20)\n",
      "memory usage: 459.4+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2800 entries, 1 to 2800\n",
      "Data columns (total 19 columns):\n",
      " #   Column                             Non-Null Count  Dtype \n",
      "---  ------                             --------------  ----- \n",
      " 0   start                              2800 non-null   object\n",
      " 1   content                            2800 non-null   object\n",
      " 2   direct_prompt                      2706 non-null   object\n",
      " 3   llm_type                           2800 non-null   object\n",
      " 4   domain                             2800 non-null   object\n",
      " 5   prompt_few_shot                    2545 non-null   object\n",
      " 6   prompt_SICO                        2696 non-null   object\n",
      " 7   paraphrase_polish_human            2735 non-null   object\n",
      " 8   paraphrase_polish_llm              2738 non-null   object\n",
      " 9   adversarial_character_human        2800 non-null   object\n",
      " 10  adversarial_character_llm          2706 non-null   object\n",
      " 11  adversarial_word_human             2800 non-null   object\n",
      " 12  adversarial_word_llm               2706 non-null   object\n",
      " 13  adversarial_character_word_human   2800 non-null   object\n",
      " 14  adversarial_character_word_llm     2706 non-null   object\n",
      " 15  paraphrase_back_translation_human  2800 non-null   object\n",
      " 16  paraphrase_back_translation_llm    2706 non-null   object\n",
      " 17  paraphrase_dipper_human            2793 non-null   object\n",
      " 18  paraphrase_dipper_llm              2792 non-null   object\n",
      "dtypes: object(19)\n",
      "memory usage: 437.5+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2800 entries, 1 to 2800\n",
      "Data columns (total 19 columns):\n",
      " #   Column                             Non-Null Count  Dtype \n",
      "---  ------                             --------------  ----- \n",
      " 0   summary                            2800 non-null   object\n",
      " 1   document                           2800 non-null   object\n",
      " 2   direct_prompt                      2655 non-null   object\n",
      " 3   llm_type                           2800 non-null   object\n",
      " 4   domain                             2800 non-null   object\n",
      " 5   paraphrase_polish_human            2633 non-null   object\n",
      " 6   paraphrase_polish_llm              2708 non-null   object\n",
      " 7   prompt_few_shot                    2620 non-null   object\n",
      " 8   prompt_SICO                        2648 non-null   object\n",
      " 9   adversarial_character_human        2800 non-null   object\n",
      " 10  adversarial_character_llm          2655 non-null   object\n",
      " 11  adversarial_word_human             2800 non-null   object\n",
      " 12  adversarial_word_llm               2655 non-null   object\n",
      " 13  adversarial_character_word_human   2800 non-null   object\n",
      " 14  adversarial_character_word_llm     2655 non-null   object\n",
      " 15  paraphrase_back_translation_human  2800 non-null   object\n",
      " 16  paraphrase_back_translation_llm    2655 non-null   object\n",
      " 17  paraphrase_dipper_human            2800 non-null   object\n",
      " 18  paraphrase_dipper_llm              2800 non-null   object\n",
      "dtypes: object(19)\n",
      "memory usage: 437.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for _, _df in domain_dfs.items():\n",
    "    print(_df.info())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-15T09:16:28.408344800Z",
     "start_time": "2025-08-15T09:16:28.361576200Z"
    }
   },
   "id": "a416484d6f79d6ca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
